{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hExKCzh6doIW"
      },
      "source": [
        "# Lab 3 - Comparing Classification Models\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HixoFOoCIJ7V"
      },
      "source": [
        "In this session, we demonstrate how to solve a text classification task using simple \n",
        "feedforward neural network classifier. We will use IMDB Large Movie Review Dataset to train a binary classification model, able to predict whether a review is positive or negative. First, our network takes one-hot word vectors as input, averages them to make one vector and trains a \n",
        "fully-connected layer to predict the output. In the second part, we replace the one-hot vectors with the word embeddings and add a layer to see how much that improves the performance.\n",
        "\n",
        "We are going to use Keras Sequential API in this session. The Sequential API allows you to make models layer-by-layer. But it is not straightforward to define models where layers connect to more than just the previous and next layers. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m8fpBfhBpupy"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras as tk\n",
        "import numpy as np\n",
        "from keras.layers import Lambda, GlobalAveragePooling1D, Dense, Embedding\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqvPQvgvPv1W"
      },
      "source": [
        "### Downloading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EundMtGPpCdf"
      },
      "source": [
        "The dataset we will be using is the IMDB Large Movie Review Dataset, which consists of 50000 labeled movie reviews. These are split into 25,000 reviews for training and 25,000 reviews for testing. The  dataset contains an even number of positive and negative reviews, so randomly guessing yields 50% accuracy. The data is preprocessed. For text classification, it is ususal to limit the size of the vocabulary to stop the dataset from becoming too sparse, creating possible overfitting. We keep the top 10,000 most frequently occurring words in the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyuSzkafqNca",
        "outputId": "44178aaa-4622-43e9-da72-f632c3cffb2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "imdb = tk.datasets.imdb\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U4iCV9-rmay"
      },
      "source": [
        "We now can start playing around with the data, letâ€™s first see the length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-gjWRAuqg5s",
        "outputId": "3e5f3f19-8d42-412a-a70d-a8a2049dd8c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training entries: 25000, labels: 25000\n"
          ]
        }
      ],
      "source": [
        "print(\"Training entries: {}, labels: {}\".format(len(X_train), len(y_train)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTRZrpcyr-4x"
      },
      "source": [
        "The  reviews have been converted to integers and each integer represents a  word in a dictionary. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "79Ev72Kgq4XL",
        "outputId": "6c5f7159-a1cc-4bdb-9d5d-8cdbf37e1205"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9a42d56787d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        " X_train[0][:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tvuu4KhStqei"
      },
      "source": [
        "We can convert integers back to words by querying a dictionary object that contains the integer to string mapping:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMCH1OoDrSNR",
        "outputId": "409dc45e-a5db-4fa0-a2ee-2d255de02d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "word_index = imdb.get_word_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IreFXgruZot"
      },
      "source": [
        "Index 1 represents the beginning of the sentence and the index 2 is assigned to all unknown tokens. Index 0 will be used for padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "abIb7Fe5u3GQ"
      },
      "outputs": [],
      "source": [
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  \n",
        "word_index[\"<UNUSED>\"] = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TnnSuspvC5b"
      },
      "source": [
        "To reverse key and values in a dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nKOiVVXQu-_I"
      },
      "outputs": [],
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmTJEm8xvUvW"
      },
      "source": [
        "To view a word:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "SqN5jgVKvJJZ",
        "outputId": "d1b97293-a527-4b53-966a-a84dc658f2af"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'you'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reverse_word_index[25]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6QjrzgVvrYn"
      },
      "source": [
        "And to recreate the whole sentence from our training data we define decode_review:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvrKeMgxvWlv"
      },
      "outputs": [],
      "source": [
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "Sxg4YA_NvdRg",
        "outputId": "0a8e4a43-f9e7-42dd-c3cc-736f77d0ccd8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<START> french horror cinema has seen something of a revival over the last couple of years with great films such as inside and <UNK> romance <UNK> on to the scene <UNK> <UNK> the revival just slightly but stands head and shoulders over most modern horror titles and is surely one of the best french horror films ever made <UNK> was obviously shot on a low budget but this is made up for in far more ways than one by the originality of the film and this in turn is <UNK> by the excellent writing and acting that ensure the film is a winner the plot focuses on two main ideas prison and black magic the central character is a man named <UNK> sent to prison for fraud he is put in a cell with three others the quietly insane <UNK> body building <UNK> marcus and his retarded boyfriend daisy after a short while in the cell together they stumble upon a hiding place in the wall that contains an old <UNK> after <UNK> part of it they soon realise its magical powers and realise they may be able to use it to break through the prison walls br br black magic is a very interesting topic and i'm actually quite surprised that there aren't more films based on it as there's so much scope for things to do with it it's fair to say that <UNK> makes the best of it's <UNK> as despite it's <UNK> the film never actually feels restrained and manages to flow well throughout director eric <UNK> provides a great atmosphere for the film the fact that most of it takes place inside the central prison cell <UNK> that the film feels very claustrophobic and this immensely benefits the central idea of the prisoners wanting to use magic to break out of the cell it's very easy to get behind them it's often said that the unknown is the thing that really <UNK> people and this film proves that as the director <UNK> that we can never really be sure of exactly what is round the corner and this helps to ensure that <UNK> actually does manage to be quite frightening the film is memorable for a lot of reasons outside the central plot the characters are all very interesting in their own way and the fact that the book itself almost takes on its own character is very well done anyone worried that the film won't deliver by the end won't be disappointed either as the ending both makes sense and manages to be quite horrifying overall <UNK> is a truly great horror film and one of the best of the decade highly recommended viewing\""
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decode_review(X_train[10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8gIzXncfaJK"
      },
      "source": [
        "### Creating One-hot word vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9W4yb3rv_E0"
      },
      "source": [
        "It is  common to use one-hot representation as input in Natural Language Processing tasks. In Keras, the Embedding layer takes an index as an input and convert it to one-hot vector with the length of the vocabulary size. Then multiplies these vectors by a normal weight matrix. But there is no way to only get a one-hot vector as the output of a layer in Keras. To solve this we use Lambda() layer and a function that creates the one-hot layer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPO_pK9zH4C5"
      },
      "outputs": [],
      "source": [
        "def OneHot(input_dim=None, input_length=None):\n",
        "    \n",
        "    if input_dim is None or input_length is None:\n",
        "        raise TypeError(\"input_dim or input_length is not set\")\n",
        "\n",
        "    \n",
        "    def _one_hot(x, num_classes):\n",
        "        return K.one_hot(K.cast(x, 'uint8'),\n",
        "                          num_classes=num_classes)\n",
        "\n",
        "    return Lambda(_one_hot,\n",
        "                  arguments={'num_classes': input_dim},\n",
        "                  input_shape=(input_length,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "364d3MAw0ez9"
      },
      "source": [
        "input_dim refers to the length of the one-hot vector and input_length refers to the length of the input sequence. Since the input to K.one_hot should be an integer tensor, we cast x to one (Keras passes around float tensors by default).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHz76GNA2M4r"
      },
      "source": [
        " Each text sequence has in most cases different length of words. Here, we fill sequences with a pad token (0) to fit the size. This special tokens is then masked not to be accounted in averaging, loss calculation etc. We set the maximum length to 256."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9G_o7PsvgSFt"
      },
      "source": [
        "### Preparing input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jiFn7sd_wF5j"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 10000\n",
        "MAX_SEQUENCE_LENGTH = 256\n",
        "\n",
        "X_train_enc = tk.preprocessing.sequence.pad_sequences(X_train,\n",
        "                                                        value=word_index[\"<PAD>\"],\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=256)\n",
        "\n",
        "X_test_enc = tk.preprocessing.sequence.pad_sequences(X_test,\n",
        "                                                       value=word_index[\"<PAD>\"],\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcjFH1wKF_7d"
      },
      "source": [
        "And to view a padded review:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwH4dcfW_a18",
        "outputId": "768115fd-a4c9-4e9d-bb33-21c247908e71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[   1  194 1153  194 8255   78  228    5    6 1463 4369 5012  134   26\n",
            "    4  715    8  118 1634   14  394   20   13  119  954  189  102    5\n",
            "  207  110 3103   21   14   69  188    8   30   23    7    4  249  126\n",
            "   93    4  114    9 2300 1523    5  647    4  116    9   35 8163    4\n",
            "  229    9  340 1322    4  118    9    4  130 4901   19    4 1002    5\n",
            "   89   29  952   46   37    4  455    9   45   43   38 1543 1905  398\n",
            "    4 1649   26 6853    5  163   11 3215    2    4 1153    9  194  775\n",
            "    7 8255    2  349 2637  148  605    2 8003   15  123  125   68    2\n",
            " 6853   15  349  165 4362   98    5    4  228    9   43    2 1157   15\n",
            "  299  120    5  120  174   11  220  175  136   50    9 4373  228 8255\n",
            "    5    2  656  245 2350    5    4 9837  131  152  491   18    2   32\n",
            " 7464 1212   14    9    6  371   78   22  625   64 1382    9    8  168\n",
            "  145   23    4 1690   15   16    4 1355    5   28    6   52  154  462\n",
            "   33   89   78  285   16  145   95    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "\n",
            "Length:  25000\n"
          ]
        }
      ],
      "source": [
        "print(X_train_enc[1])\n",
        "print('\\nLength: ',len(X_train_enc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1zcxFwNGepA"
      },
      "source": [
        "Now we want to build the neural network model. We  are going to have a hidden layer with 16 hidden units. \n",
        "\n",
        "First, we want to transform each index to an embedded vector and then average all vectors to a single one. It has been showed that unweighted average of word vectors outperforms many complicated networks that model semantic and syntactic compositionality. As an example you can take a look at this: (http://anthology.aclweb.org/P/P15/P15-1162.pdf)\n",
        "\n",
        "To average we need to ignore padded zeros:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Yi04MLIvJOGZ"
      },
      "outputs": [],
      "source": [
        "class GlobalAveragePooling1DMasked(GlobalAveragePooling1D):\n",
        "    def call(self, x, mask=None):\n",
        "        if mask != None:\n",
        "            return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
        "        else:\n",
        "            return super().call(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whgIIB5ggjna"
      },
      "source": [
        "# Model 1: Neural averaging network using one-hot vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlOLnlnSJgrU"
      },
      "source": [
        "The first layer is an one-hot layer. The second layer is to compute average on all word vectors in a sentence without considering padding. The  output vector is piped through a fully-connected layer. The last layer is connected with a single output node with the sigmoid activation function. The final value is a float between 0 and 1. \n",
        "The vocabulary count of the movie reviews (10000) is used as the input shape. At the end we visualize the model summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iKuSmbfohY1n"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input#, Dot , Activation\n",
        "from keras.layers.core import Dense#, Reshape\n",
        "#from keras.layers.embeddings import Embedding\n",
        "from keras.models import Model\n",
        "#from keras.utils.vis_utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh-Chol8hAww",
        "outputId": "b985afc1-22b5-4b07-f16d-d103f716fc3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 256)]             0         \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 256, 10000)        0         \n",
            "                                                                 \n",
            " global_average_pooling1d_ma  (None, 10000)            0         \n",
            " sked (GlobalAveragePooling1                                     \n",
            " DMasked)                                                        \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                160016    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 160,033\n",
            "Trainable params: 160,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "\n",
        "#Input layer. \n",
        "target_word = Input((MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "\n",
        "#One hot vectors. \n",
        "one_hot_vec= OneHot(input_dim=VOCAB_SIZE, input_length=MAX_SEQUENCE_LENGTH)(target_word)\n",
        "\n",
        "#Average one hot vectors. \n",
        "global_pol = GlobalAveragePooling1DMasked()(one_hot_vec)\n",
        "\n",
        "#Hidden layer. \n",
        "hidden_layer = Dense(16, activation=\"tanh\")(global_pol)  #Check between tahn, relu... keep the best. \n",
        "\n",
        "#Output\n",
        "output = Dense(1,activation=\"sigmoid\")(hidden_layer)\n",
        "\n",
        "#Initialise model \n",
        "model = Model(inputs=target_word, outputs=[output]) \n",
        "\n",
        "model.summary() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Mz96xpCgvTj"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3HbW_IKLqwT"
      },
      "source": [
        "To compile the model we need a loss function and an optimizer. We use binary_crossentropy loss function which is just a special case of categorical cross entropy. We also use Adam optimizer that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data. You can read more about it here:\n",
        "(https://arxiv.org/abs/1412.6980v8\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh1PWTNMxjUw"
      },
      "outputs": [],
      "source": [
        "# your code goes here\n",
        "\n",
        "#Compile model. \n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1jwQQqCN5Ia"
      },
      "source": [
        "When training, we want to check the accuracy of the model on data it hasn't seen before. So we create a validation set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5lAqzQlxjSM"
      },
      "outputs": [],
      "source": [
        "X_val = np.array(X_train_enc[:10000])\n",
        "partial_X_train = np.array(X_train_enc[10000:])\n",
        "\n",
        "y_val = np.array(y_train[:10000])\n",
        "partial_y_train = np.array(y_train[10000:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8Kpo5G3OJEY"
      },
      "source": [
        "Then we start to train the model for 40 epochs in mini-batches of 512 samples and monitor the model's loss and accuracy on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "99_z39KAxjPi",
        "outputId": "83242c39-08da-48a5-e776-4a4770226289"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "30/30 [==============================] - 6s 95ms/step - loss: 0.6926 - accuracy: 0.5030 - val_loss: 0.6918 - val_accuracy: 0.4957\n",
            "Epoch 2/40\n",
            "30/30 [==============================] - 3s 89ms/step - loss: 0.6906 - accuracy: 0.6263 - val_loss: 0.6897 - val_accuracy: 0.6461\n",
            "Epoch 3/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.6882 - accuracy: 0.5711 - val_loss: 0.6870 - val_accuracy: 0.6545\n",
            "Epoch 4/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.6849 - accuracy: 0.6643 - val_loss: 0.6835 - val_accuracy: 0.6639\n",
            "Epoch 5/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.6809 - accuracy: 0.6670 - val_loss: 0.6795 - val_accuracy: 0.6658\n",
            "Epoch 6/40\n",
            "30/30 [==============================] - 3s 87ms/step - loss: 0.6762 - accuracy: 0.6760 - val_loss: 0.6746 - val_accuracy: 0.6707\n",
            "Epoch 7/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.6708 - accuracy: 0.6778 - val_loss: 0.6698 - val_accuracy: 0.6739\n",
            "Epoch 8/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.6651 - accuracy: 0.6827 - val_loss: 0.6638 - val_accuracy: 0.6775\n",
            "Epoch 9/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.6592 - accuracy: 0.6823 - val_loss: 0.6582 - val_accuracy: 0.6810\n",
            "Epoch 10/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.6527 - accuracy: 0.6885 - val_loss: 0.6523 - val_accuracy: 0.6833\n",
            "Epoch 11/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.6463 - accuracy: 0.6913 - val_loss: 0.6461 - val_accuracy: 0.6845\n",
            "Epoch 12/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.6399 - accuracy: 0.6931 - val_loss: 0.6405 - val_accuracy: 0.6886\n",
            "Epoch 13/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.6336 - accuracy: 0.6941 - val_loss: 0.6343 - val_accuracy: 0.6922\n",
            "Epoch 14/40\n",
            "30/30 [==============================] - 3s 87ms/step - loss: 0.6271 - accuracy: 0.6988 - val_loss: 0.6288 - val_accuracy: 0.6980\n",
            "Epoch 15/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.6209 - accuracy: 0.7015 - val_loss: 0.6227 - val_accuracy: 0.6985\n",
            "Epoch 16/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.6151 - accuracy: 0.7033 - val_loss: 0.6177 - val_accuracy: 0.7023\n",
            "Epoch 17/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.6093 - accuracy: 0.7056 - val_loss: 0.6114 - val_accuracy: 0.7047\n",
            "Epoch 18/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.6034 - accuracy: 0.7115 - val_loss: 0.6063 - val_accuracy: 0.7108\n",
            "Epoch 19/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.5978 - accuracy: 0.7140 - val_loss: 0.6028 - val_accuracy: 0.7068\n",
            "Epoch 20/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.5927 - accuracy: 0.7169 - val_loss: 0.5967 - val_accuracy: 0.7140\n",
            "Epoch 21/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.5873 - accuracy: 0.7215 - val_loss: 0.5915 - val_accuracy: 0.7071\n",
            "Epoch 22/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.5827 - accuracy: 0.7215 - val_loss: 0.5865 - val_accuracy: 0.7182\n",
            "Epoch 23/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.5776 - accuracy: 0.7249 - val_loss: 0.5821 - val_accuracy: 0.7195\n",
            "Epoch 24/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.5730 - accuracy: 0.7254 - val_loss: 0.5779 - val_accuracy: 0.7243\n",
            "Epoch 25/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.5688 - accuracy: 0.7259 - val_loss: 0.5741 - val_accuracy: 0.7255\n",
            "Epoch 26/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.5647 - accuracy: 0.7308 - val_loss: 0.5699 - val_accuracy: 0.7267\n",
            "Epoch 27/40\n",
            "30/30 [==============================] - 3s 89ms/step - loss: 0.5611 - accuracy: 0.7311 - val_loss: 0.5666 - val_accuracy: 0.7237\n",
            "Epoch 28/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.5570 - accuracy: 0.7346 - val_loss: 0.5628 - val_accuracy: 0.7299\n",
            "Epoch 29/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.5536 - accuracy: 0.7376 - val_loss: 0.5599 - val_accuracy: 0.7297\n",
            "Epoch 30/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.5505 - accuracy: 0.7393 - val_loss: 0.5565 - val_accuracy: 0.7324\n",
            "Epoch 31/40\n",
            "30/30 [==============================] - 3s 89ms/step - loss: 0.5472 - accuracy: 0.7409 - val_loss: 0.5535 - val_accuracy: 0.7347\n",
            "Epoch 32/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.5442 - accuracy: 0.7435 - val_loss: 0.5511 - val_accuracy: 0.7355\n",
            "Epoch 33/40\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 0.5415 - accuracy: 0.7435 - val_loss: 0.5485 - val_accuracy: 0.7375\n",
            "Epoch 34/40\n",
            "30/30 [==============================] - 3s 89ms/step - loss: 0.5390 - accuracy: 0.7445 - val_loss: 0.5463 - val_accuracy: 0.7394\n",
            "Epoch 35/40\n",
            "30/30 [==============================] - 3s 89ms/step - loss: 0.5367 - accuracy: 0.7453 - val_loss: 0.5442 - val_accuracy: 0.7395\n",
            "Epoch 36/40\n",
            "30/30 [==============================] - 3s 89ms/step - loss: 0.5343 - accuracy: 0.7453 - val_loss: 0.5436 - val_accuracy: 0.7358\n",
            "Epoch 37/40\n",
            "30/30 [==============================] - 3s 89ms/step - loss: 0.5324 - accuracy: 0.7473 - val_loss: 0.5403 - val_accuracy: 0.7406\n",
            "Epoch 38/40\n",
            "30/30 [==============================] - 3s 89ms/step - loss: 0.5308 - accuracy: 0.7459 - val_loss: 0.5392 - val_accuracy: 0.7394\n",
            "Epoch 39/40\n",
            "30/30 [==============================] - 3s 89ms/step - loss: 0.5288 - accuracy: 0.7474 - val_loss: 0.5363 - val_accuracy: 0.7439\n",
            "Epoch 40/40\n",
            "30/30 [==============================] - 3s 89ms/step - loss: 0.5267 - accuracy: 0.7497 - val_loss: 0.5350 - val_accuracy: 0.7420\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(partial_X_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_9a_rybhG5J"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYLH8kOgOo9W"
      },
      "source": [
        "To evaulate the model on test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFMt2Q7b3taP",
        "outputId": "e9bbc860-2787-482d-965a-3945a5e3a1b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 4s 5ms/step - loss: 0.5346 - accuracy: 0.7424\n"
          ]
        }
      ],
      "source": [
        "results = model.evaluate(X_test_enc, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RrKiPHcAmQU",
        "outputId": "c3d57e6b-7bb0-4592-b0c0-3477664c4a98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.5346466898918152, 0.7423999905586243]\n"
          ]
        }
      ],
      "source": [
        "print(results)\n",
        "# loss, accuracay "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW7IpHxMO6qp"
      },
      "source": [
        "Our first model accuracy using one-hot vectors is ~68%\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwZk_yoWhPJB"
      },
      "source": [
        "### Plotting the accuracy graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIDPH1J7PMzN"
      },
      "source": [
        "To plot a graph of accuracy and loss over time we can use Matplotlib:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS9k2vvSAqB7",
        "outputId": "a0a78ed2-49a7-480c-d687-c76c4aa70771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting plot_keras_history\n",
            "  Downloading plot_keras_history-1.1.30.tar.gz (8.6 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from plot_keras_history) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from plot_keras_history) (1.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from plot_keras_history) (1.4.1)\n",
            "Collecting sanitize_ml_labels>=1.0.28\n",
            "  Downloading sanitize_ml_labels-1.0.29.tar.gz (7.4 kB)\n",
            "Collecting compress_json\n",
            "  Downloading compress_json-1.0.4.tar.gz (4.7 kB)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (1.21.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (3.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->plot_keras_history) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->plot_keras_history) (2018.9)\n",
            "Building wheels for collected packages: plot-keras-history, sanitize-ml-labels, compress-json\n",
            "  Building wheel for plot-keras-history (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for plot-keras-history: filename=plot_keras_history-1.1.30-py3-none-any.whl size=8794 sha256=2d1d1977ef766e9ce81696c15d7949c1bb3ade08e48575999eccc17bb84b6542\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/60/47/8c5aa37c06be5e97879ec467bc2e6a30b315d95f662c63a503\n",
            "  Building wheel for sanitize-ml-labels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sanitize-ml-labels: filename=sanitize_ml_labels-1.0.29-py3-none-any.whl size=7878 sha256=e7d5fd49b5c28aa77a1c37a9cbeaec3a117228dbb321d560a889ca4aac16c97b\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/f5/71/d1c459da10abec864a1979b449edbf37d4a82ab3e38a3625a8\n",
            "  Building wheel for compress-json (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compress-json: filename=compress_json-1.0.4-py3-none-any.whl size=4588 sha256=f99287fa0c0b8636c48ae2b5d6cb96cb5d270a623ba92034ff8f4697e95b0174\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/ef/1e/5d403c5632b0462471a8d26049d0c138134d0255ec60ce4c14\n",
            "Successfully built plot-keras-history sanitize-ml-labels compress-json\n",
            "Installing collected packages: compress-json, sanitize-ml-labels, plot-keras-history\n",
            "Successfully installed compress-json-1.0.4 plot-keras-history-1.1.30 sanitize-ml-labels-1.0.29\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "!pip install plot_keras_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OGe5Oq12n0wX"
      },
      "outputs": [],
      "source": [
        "from plot_keras_history import plot_history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plot_history(history.history, path=\"standard.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7OwOQw4h8RX"
      },
      "source": [
        "# Model 2: Neural averaging network using embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-QzOMO_P4jc"
      },
      "source": [
        "Now instead of one-hot vectors, we want to use embedding. We change our first layer in model1 to an Embedding layer. This layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: (batch, sequence, embedding)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFrCsL-NBFVL",
        "outputId": "1271bc10-569b-47d1-97a0-93133dc38878"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 256)]             0         \n",
            "                                                                 \n",
            " target_embed_layer (Embeddi  (None, 256, 256)         2560000   \n",
            " ng)                                                             \n",
            "                                                                 \n",
            " global_average_pooling1d_ma  (None, 256)              0         \n",
            " sked_1 (GlobalAveragePoolin                                     \n",
            " g1DMasked)                                                      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                4112      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,564,129\n",
            "Trainable params: 2,564,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/40\n",
            "30/30 [==============================] - 1s 16ms/step - loss: 0.7133 - accuracy: 0.5035 - val_loss: 0.6939 - val_accuracy: 0.4947\n",
            "Epoch 2/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.6880 - accuracy: 0.5709 - val_loss: 0.6822 - val_accuracy: 0.6482\n",
            "Epoch 3/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.6743 - accuracy: 0.7189 - val_loss: 0.6654 - val_accuracy: 0.7550\n",
            "Epoch 4/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.6480 - accuracy: 0.7721 - val_loss: 0.6325 - val_accuracy: 0.7641\n",
            "Epoch 5/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.6042 - accuracy: 0.7817 - val_loss: 0.5858 - val_accuracy: 0.7771\n",
            "Epoch 6/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.5491 - accuracy: 0.8028 - val_loss: 0.5321 - val_accuracy: 0.8016\n",
            "Epoch 7/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.4905 - accuracy: 0.8311 - val_loss: 0.4790 - val_accuracy: 0.8250\n",
            "Epoch 8/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.4358 - accuracy: 0.8547 - val_loss: 0.4349 - val_accuracy: 0.8387\n",
            "Epoch 9/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.3885 - accuracy: 0.8699 - val_loss: 0.3960 - val_accuracy: 0.8545\n",
            "Epoch 10/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.3498 - accuracy: 0.8834 - val_loss: 0.3690 - val_accuracy: 0.8621\n",
            "Epoch 11/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.3186 - accuracy: 0.8931 - val_loss: 0.3476 - val_accuracy: 0.8670\n",
            "Epoch 12/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.2929 - accuracy: 0.9016 - val_loss: 0.3317 - val_accuracy: 0.8725\n",
            "Epoch 13/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.2700 - accuracy: 0.9094 - val_loss: 0.3188 - val_accuracy: 0.8755\n",
            "Epoch 14/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.2505 - accuracy: 0.9164 - val_loss: 0.3103 - val_accuracy: 0.8775\n",
            "Epoch 15/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.2333 - accuracy: 0.9225 - val_loss: 0.3015 - val_accuracy: 0.8797\n",
            "Epoch 16/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.2178 - accuracy: 0.9284 - val_loss: 0.2968 - val_accuracy: 0.8801\n",
            "Epoch 17/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.2033 - accuracy: 0.9338 - val_loss: 0.2924 - val_accuracy: 0.8817\n",
            "Epoch 18/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1905 - accuracy: 0.9404 - val_loss: 0.2894 - val_accuracy: 0.8833\n",
            "Epoch 19/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1786 - accuracy: 0.9451 - val_loss: 0.2888 - val_accuracy: 0.8821\n",
            "Epoch 20/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1675 - accuracy: 0.9497 - val_loss: 0.2861 - val_accuracy: 0.8847\n",
            "Epoch 21/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1570 - accuracy: 0.9539 - val_loss: 0.2861 - val_accuracy: 0.8863\n",
            "Epoch 22/40\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.1478 - accuracy: 0.9583 - val_loss: 0.2870 - val_accuracy: 0.8858\n",
            "Epoch 23/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1390 - accuracy: 0.9605 - val_loss: 0.2890 - val_accuracy: 0.8853\n",
            "Epoch 24/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1308 - accuracy: 0.9645 - val_loss: 0.2918 - val_accuracy: 0.8845\n",
            "Epoch 25/40\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.1251 - accuracy: 0.9659 - val_loss: 0.2942 - val_accuracy: 0.8834\n",
            "Epoch 26/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1170 - accuracy: 0.9691 - val_loss: 0.2974 - val_accuracy: 0.8819\n",
            "Epoch 27/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1096 - accuracy: 0.9724 - val_loss: 0.2986 - val_accuracy: 0.8830\n",
            "Epoch 28/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.1039 - accuracy: 0.9741 - val_loss: 0.3021 - val_accuracy: 0.8818\n",
            "Epoch 29/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0995 - accuracy: 0.9752 - val_loss: 0.3079 - val_accuracy: 0.8805\n",
            "Epoch 30/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0929 - accuracy: 0.9788 - val_loss: 0.3110 - val_accuracy: 0.8797\n",
            "Epoch 31/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0869 - accuracy: 0.9811 - val_loss: 0.3156 - val_accuracy: 0.8794\n",
            "Epoch 32/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0825 - accuracy: 0.9828 - val_loss: 0.3211 - val_accuracy: 0.8776\n",
            "Epoch 33/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0775 - accuracy: 0.9845 - val_loss: 0.3253 - val_accuracy: 0.8778\n",
            "Epoch 34/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0734 - accuracy: 0.9853 - val_loss: 0.3297 - val_accuracy: 0.8763\n",
            "Epoch 35/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0698 - accuracy: 0.9866 - val_loss: 0.3363 - val_accuracy: 0.8750\n",
            "Epoch 36/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0657 - accuracy: 0.9879 - val_loss: 0.3412 - val_accuracy: 0.8744\n",
            "Epoch 37/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0629 - accuracy: 0.9891 - val_loss: 0.3464 - val_accuracy: 0.8756\n",
            "Epoch 38/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0589 - accuracy: 0.9903 - val_loss: 0.3521 - val_accuracy: 0.8742\n",
            "Epoch 39/40\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 0.0559 - accuracy: 0.9911 - val_loss: 0.3579 - val_accuracy: 0.8741\n",
            "Epoch 40/40\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.0527 - accuracy: 0.9927 - val_loss: 0.3646 - val_accuracy: 0.8733\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "\n",
        "#Input layer. \n",
        "target_word = Input((MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "\n",
        "#Embedding layer. \n",
        "target_embedding = Embedding(VOCAB_SIZE, MAX_SEQUENCE_LENGTH, name='target_embed_layer',\n",
        "                        \tembeddings_initializer='glorot_uniform',\n",
        "                         \tinput_length=1)(target_word)\n",
        "\n",
        "#Average one hot vectors. \n",
        "global_pol = GlobalAveragePooling1DMasked()(target_embedding)\n",
        "\n",
        "#Hidden layer. \n",
        "hidden_layer = Dense(16, activation=\"sigmoid\")(global_pol)  #Check between tahn, relu... keep the best. \n",
        "\n",
        "#Output\n",
        "output = Dense(1,activation=\"sigmoid\")(hidden_layer)\n",
        "\n",
        "#Initialise model \n",
        "model = Model(inputs=target_word, outputs=[output]) \n",
        "\n",
        "model.summary() \n",
        "\n",
        "\n",
        "#Compile model. \n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(partial_X_train, partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEb2LIUXPkOa",
        "outputId": "0fab4bee-6b38-4006-b37e-0f2f0d227f0c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_history(history.history, path=\"standard.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4zIPJDcTPq3",
        "outputId": "a93679c8-5434-49ba-b2b0-c27099520a7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 7s 9ms/step - loss: 0.3564 - accuracy: 0.8674: 0s - loss: 0.354\n"
          ]
        }
      ],
      "source": [
        "results = model.evaluate(X_test_enc, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waS96edDTRyL",
        "outputId": "3a13a203-4998-4575-fa12-d608209a50a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.3563762605190277, 0.8674399852752686]\n"
          ]
        }
      ],
      "source": [
        "print (results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FBpTc_rXGvQ"
      },
      "source": [
        "The accuracy of model2 is 87%. Using Embedding layer instead of one-hot layer improved the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--020hfG6rN2"
      },
      "source": [
        "# Model 3: Using pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GdY2-64YG1B"
      },
      "source": [
        "### Preparing pre-trained word embeddings (GLOVE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4gBeOyi4gkM"
      },
      "source": [
        "The Embedding layer can be used to load a pre-trained word embedding model. We are going to use GloVe embeddings, which you can read about it here (https://nlp.stanford.edu/projects/glove/). GloVe stands for \"Global Vectors for Word Representation\". It's a somewhat popular embedding technique based on factorizing a matrix of word co-occurence statistics. You can download GloVe and we can seed the Keras Embedding layer with weights from the pre-trained embedding for the words in your dataset.\n",
        "First, we need to read GloVe and map words to GloVe:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "f_PypdqG9Iis"
      },
      "outputs": [],
      "source": [
        "def readGloveFile(gloveFile):\n",
        "    with open(gloveFile, 'r') as f:\n",
        "        wordToGlove = {}  \n",
        "        wordToIndex = {}  \n",
        "        indexToWord = {}  \n",
        "\n",
        "        for line in f:\n",
        "            record = line.strip().split()\n",
        "            token = record[0] \n",
        "            wordToGlove[token] = np.array(record[1:], dtype=np.float64) \n",
        "            \n",
        "            \n",
        "        tokens = sorted(wordToGlove.keys())\n",
        "        for idx, tok in enumerate(tokens):\n",
        "            kerasIdx = idx + 1  \n",
        "            wordToIndex[tok] = kerasIdx \n",
        "            indexToWord[kerasIdx] = tok \n",
        "\n",
        "    return wordToIndex, indexToWord, wordToGlove"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcIZ3dq59bCh"
      },
      "source": [
        "Now, we create our pre-trained Embedding layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gembn7VM3ex8"
      },
      "outputs": [],
      "source": [
        "from keras.initializers import Constant\n",
        "\n",
        "def createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable):\n",
        "    vocabLen = len(wordToIndex) + 1  \n",
        "    embDim = next(iter(wordToGlove.values())).shape[0]  \n",
        "   \n",
        "    embeddingMatrix = np.zeros((vocabLen, embDim))  \n",
        "    for word, index in wordToIndex.items():\n",
        "        embeddingMatrix[index, :] = wordToGlove[word] \n",
        "\n",
        "    embeddingLayer = Embedding(vocabLen, embDim, embeddings_initializer=Constant(embeddingMatrix), trainable=isTrainable, name='GloVe_Embeddings')\n",
        "    return embeddingLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OC1wuctdFvA",
        "outputId": "8ffc6883-93a6-429f-e6f9-822e308f383d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-07 17:34:25--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-03-07 17:34:25--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-03-07 17:34:26--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: â€˜glove.6B.zipâ€™\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.14MB/s    in 2m 42s  \n",
            "\n",
            "2022-03-07 17:37:08 (5.09 MB/s) - â€˜glove.6B.zipâ€™ saved [862182613/862182613]\n",
            "\n",
            "Archive:  /content/glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip '/content/glove.6B.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGxciLK4-xOr"
      },
      "source": [
        "We freeze the weights. To create the model: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZCPUM0W_Drc",
        "outputId": "49a697e0-3091-4f06-ee40-64c90569ec79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of Embedding:  300\n"
          ]
        }
      ],
      "source": [
        "# wordToIndex,indexToWord,wordToGlove=readGloveFile('/content/glove.6B.50d.txt')\n",
        "# wordToIndex,indexToWord,wordToGlove=readGloveFile('/content/glove.6B.100d.txt')\n",
        "wordToIndex,indexToWord,wordToGlove=readGloveFile('/content/glove.6B.300d.txt')\n",
        "\n",
        "# vocabLen = len(wordToIndex) + 1 \n",
        "\n",
        "EMBED_SIZE = next(iter(wordToGlove.values())).shape[0]\n",
        "print('Size of Embedding: ',EMBED_SIZE)\n",
        "\n",
        "embeddingLayer=createPretrainedEmbeddingLayer(wordToGlove,wordToIndex,isTrainable=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdZ4nl08vp9A"
      },
      "source": [
        "## Model 3-1: Neural bag of words using pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gyCwXFj_R5w"
      },
      "source": [
        "Now we change our model to use GloVe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFGhxUiYrYl3"
      },
      "outputs": [],
      "source": [
        "# your code goes here\n",
        "\n",
        "#Input layer. \n",
        "target_word = Input((MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "\n",
        "#Embedding layer. \n",
        "target_embedding = embeddingLayer(target_word)\n",
        "\n",
        "#Average one hot vectors. \n",
        "global_pol = GlobalAveragePooling1DMasked()(target_embedding)\n",
        "\n",
        "#Hidden layer. \n",
        "hidden_layer = Dense(16, activation=\"sigmoid\")(global_pol)  #Check between tahn, relu... keep the best. \n",
        "\n",
        "#Output\n",
        "output = Dense(1,activation=\"sigmoid\")(hidden_layer)\n",
        "\n",
        "#Initialise model \n",
        "model = Model(inputs=target_word, outputs=[output]) \n",
        "\n",
        "model.summary() \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r6sj_FnOD9Sb",
        "outputId": "a5862a3e-373e-40d7-de3b-5f43daf360dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "30/30 [==============================] - 4s 125ms/step - loss: 0.5313 - accuracy: 0.9361 - val_loss: 0.4503 - val_accuracy: 0.8635\n",
            "Epoch 2/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.3038 - accuracy: 0.9965 - val_loss: 0.3545 - val_accuracy: 0.8642\n",
            "Epoch 3/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.1977 - accuracy: 0.9963 - val_loss: 0.3217 - val_accuracy: 0.8674\n",
            "Epoch 4/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.1421 - accuracy: 0.9973 - val_loss: 0.3131 - val_accuracy: 0.8696\n",
            "Epoch 5/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.1091 - accuracy: 0.9974 - val_loss: 0.3152 - val_accuracy: 0.8715\n",
            "Epoch 6/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0875 - accuracy: 0.9973 - val_loss: 0.3227 - val_accuracy: 0.8719\n",
            "Epoch 7/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0724 - accuracy: 0.9974 - val_loss: 0.3331 - val_accuracy: 0.8699\n",
            "Epoch 8/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0609 - accuracy: 0.9978 - val_loss: 0.3440 - val_accuracy: 0.8700\n",
            "Epoch 9/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0522 - accuracy: 0.9980 - val_loss: 0.3566 - val_accuracy: 0.8692\n",
            "Epoch 10/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0455 - accuracy: 0.9983 - val_loss: 0.3690 - val_accuracy: 0.8699\n",
            "Epoch 11/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0398 - accuracy: 0.9984 - val_loss: 0.3813 - val_accuracy: 0.8686\n",
            "Epoch 12/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0351 - accuracy: 0.9985 - val_loss: 0.3950 - val_accuracy: 0.8668\n",
            "Epoch 13/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0315 - accuracy: 0.9987 - val_loss: 0.4076 - val_accuracy: 0.8666\n",
            "Epoch 14/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0281 - accuracy: 0.9988 - val_loss: 0.4189 - val_accuracy: 0.8676\n",
            "Epoch 15/40\n",
            "30/30 [==============================] - 3s 117ms/step - loss: 0.0256 - accuracy: 0.9989 - val_loss: 0.4321 - val_accuracy: 0.8665\n",
            "Epoch 16/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0230 - accuracy: 0.9990 - val_loss: 0.4430 - val_accuracy: 0.8670\n",
            "Epoch 17/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0209 - accuracy: 0.9991 - val_loss: 0.4552 - val_accuracy: 0.8654\n",
            "Epoch 18/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0191 - accuracy: 0.9992 - val_loss: 0.4663 - val_accuracy: 0.8663\n",
            "Epoch 19/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0175 - accuracy: 0.9992 - val_loss: 0.4770 - val_accuracy: 0.8665\n",
            "Epoch 20/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0162 - accuracy: 0.9993 - val_loss: 0.4885 - val_accuracy: 0.8654\n",
            "Epoch 21/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0149 - accuracy: 0.9993 - val_loss: 0.4980 - val_accuracy: 0.8655\n",
            "Epoch 22/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0139 - accuracy: 0.9995 - val_loss: 0.5088 - val_accuracy: 0.8654\n",
            "Epoch 23/40\n",
            "30/30 [==============================] - 3s 117ms/step - loss: 0.0127 - accuracy: 0.9994 - val_loss: 0.5182 - val_accuracy: 0.8651\n",
            "Epoch 24/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0121 - accuracy: 0.9994 - val_loss: 0.5282 - val_accuracy: 0.8650\n",
            "Epoch 25/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0110 - accuracy: 0.9995 - val_loss: 0.5377 - val_accuracy: 0.8650\n",
            "Epoch 26/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0104 - accuracy: 0.9995 - val_loss: 0.5481 - val_accuracy: 0.8648\n",
            "Epoch 27/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0096 - accuracy: 0.9996 - val_loss: 0.5563 - val_accuracy: 0.8648\n",
            "Epoch 28/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0091 - accuracy: 0.9997 - val_loss: 0.5651 - val_accuracy: 0.8642\n",
            "Epoch 29/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0086 - accuracy: 0.9996 - val_loss: 0.5728 - val_accuracy: 0.8637\n",
            "Epoch 30/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0082 - accuracy: 0.9997 - val_loss: 0.5810 - val_accuracy: 0.8644\n",
            "Epoch 31/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0077 - accuracy: 0.9997 - val_loss: 0.5899 - val_accuracy: 0.8636\n",
            "Epoch 32/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0072 - accuracy: 0.9997 - val_loss: 0.5977 - val_accuracy: 0.8641\n",
            "Epoch 33/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0069 - accuracy: 0.9997 - val_loss: 0.6054 - val_accuracy: 0.8634\n",
            "Epoch 34/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0066 - accuracy: 0.9997 - val_loss: 0.6135 - val_accuracy: 0.8623\n",
            "Epoch 35/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0062 - accuracy: 0.9997 - val_loss: 0.6206 - val_accuracy: 0.8636\n",
            "Epoch 36/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0058 - accuracy: 0.9997 - val_loss: 0.6273 - val_accuracy: 0.8637\n",
            "Epoch 37/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0055 - accuracy: 0.9997 - val_loss: 0.6345 - val_accuracy: 0.8634\n",
            "Epoch 38/40\n",
            "30/30 [==============================] - 4s 119ms/step - loss: 0.0053 - accuracy: 0.9998 - val_loss: 0.6412 - val_accuracy: 0.8623\n",
            "Epoch 39/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0051 - accuracy: 0.9997 - val_loss: 0.6483 - val_accuracy: 0.8627\n",
            "Epoch 40/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0049 - accuracy: 0.9998 - val_loss: 0.6547 - val_accuracy: 0.8627\n"
          ]
        }
      ],
      "source": [
        "# Train and test the model\n",
        "\n",
        "#Compile model. \n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(partial_X_train, partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Td9SyqN4YFla",
        "outputId": "bd48309b-626c-4813-acdc-6921d58f6712"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6990 - accuracy: 0.8489\n"
          ]
        }
      ],
      "source": [
        "results = model.evaluate(X_test_enc, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_O4T2zUYLnb",
        "outputId": "7f044fa3-198d-46e7-e3ee-014bde21fc89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.6990035772323608, 0.8489199876785278]\n"
          ]
        }
      ],
      "source": [
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxJhlT3whoDE"
      },
      "source": [
        "To compare freezing and fine-tuning the pre-train embedding weights, we fine-tune the weights here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_xmeuuhhtbH",
        "outputId": "65a7e2d4-d0a2-404d-fdbf-9fc1b15c0a0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 256)]             0         \n",
            "                                                                 \n",
            " GloVe_Embeddings (Embedding  (None, 256, 300)         120000300 \n",
            " )                                                               \n",
            "                                                                 \n",
            " global_average_pooling1d_ma  (None, 300)              0         \n",
            " sked (GlobalAveragePooling1                                     \n",
            " DMasked)                                                        \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                4816      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 120,005,133\n",
            "Trainable params: 120,005,133\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "#Not frozen --> fine-tuning (isTrainable=True)\n",
        "embeddingLayer=createPretrainedEmbeddingLayer(wordToGlove,wordToIndex,isTrainable=True)\n",
        "\n",
        "#Input layer. \n",
        "target_word = Input((MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "\n",
        "#Embedding layer. \n",
        "target_embedding = embeddingLayer(target_word)\n",
        "\n",
        "#Average one hot vectors. \n",
        "global_pol = GlobalAveragePooling1DMasked()(target_embedding)\n",
        "\n",
        "#Hidden layer. \n",
        "hidden_layer = Dense(16, activation=\"sigmoid\")(global_pol)  #Check between tahn, relu... keep the best. \n",
        "\n",
        "#Output\n",
        "output = Dense(1,activation=\"sigmoid\")(hidden_layer)\n",
        "\n",
        "#Initialise model \n",
        "model3 = Model(inputs=target_word, outputs=[output]) \n",
        "\n",
        "model3.summary() \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rz8MEm6iHj6",
        "outputId": "39e63600-529b-4d6e-b2fa-d39647a2c7cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "30/30 [==============================] - 3s 82ms/step - loss: 0.6956 - accuracy: 0.5090 - val_loss: 0.6899 - val_accuracy: 0.5333\n",
            "Epoch 2/40\n",
            "30/30 [==============================] - 2s 77ms/step - loss: 0.6857 - accuracy: 0.5965 - val_loss: 0.6821 - val_accuracy: 0.6263\n",
            "Epoch 3/40\n",
            "30/30 [==============================] - 2s 75ms/step - loss: 0.6751 - accuracy: 0.6535 - val_loss: 0.6690 - val_accuracy: 0.6582\n",
            "Epoch 4/40\n",
            "30/30 [==============================] - 2s 75ms/step - loss: 0.6560 - accuracy: 0.6816 - val_loss: 0.6457 - val_accuracy: 0.6773\n",
            "Epoch 5/40\n",
            "30/30 [==============================] - 2s 79ms/step - loss: 0.6256 - accuracy: 0.7101 - val_loss: 0.6117 - val_accuracy: 0.7172\n",
            "Epoch 6/40\n",
            "30/30 [==============================] - 2s 77ms/step - loss: 0.5834 - accuracy: 0.7469 - val_loss: 0.5678 - val_accuracy: 0.7552\n",
            "Epoch 7/40\n",
            "30/30 [==============================] - 2s 80ms/step - loss: 0.5311 - accuracy: 0.7843 - val_loss: 0.5163 - val_accuracy: 0.7944\n",
            "Epoch 8/40\n",
            "30/30 [==============================] - 2s 77ms/step - loss: 0.4752 - accuracy: 0.8195 - val_loss: 0.4665 - val_accuracy: 0.8120\n",
            "Epoch 9/40\n",
            "30/30 [==============================] - 2s 82ms/step - loss: 0.4226 - accuracy: 0.8483 - val_loss: 0.4219 - val_accuracy: 0.8372\n",
            "Epoch 10/40\n",
            "30/30 [==============================] - 2s 75ms/step - loss: 0.3787 - accuracy: 0.8645 - val_loss: 0.3885 - val_accuracy: 0.8489\n",
            "Epoch 11/40\n",
            "30/30 [==============================] - 2s 77ms/step - loss: 0.3434 - accuracy: 0.8791 - val_loss: 0.3634 - val_accuracy: 0.8601\n",
            "Epoch 12/40\n",
            "30/30 [==============================] - 2s 77ms/step - loss: 0.3154 - accuracy: 0.8877 - val_loss: 0.3454 - val_accuracy: 0.8649\n",
            "Epoch 13/40\n",
            "30/30 [==============================] - 2s 80ms/step - loss: 0.2908 - accuracy: 0.8979 - val_loss: 0.3302 - val_accuracy: 0.8704\n",
            "Epoch 14/40\n",
            "30/30 [==============================] - 2s 78ms/step - loss: 0.2700 - accuracy: 0.9051 - val_loss: 0.3193 - val_accuracy: 0.8733\n",
            "Epoch 15/40\n",
            "30/30 [==============================] - 2s 77ms/step - loss: 0.2524 - accuracy: 0.9112 - val_loss: 0.3089 - val_accuracy: 0.8770\n",
            "Epoch 16/40\n",
            "30/30 [==============================] - 2s 77ms/step - loss: 0.2351 - accuracy: 0.9193 - val_loss: 0.3020 - val_accuracy: 0.8788\n",
            "Epoch 17/40\n",
            "30/30 [==============================] - 2s 75ms/step - loss: 0.2201 - accuracy: 0.9250 - val_loss: 0.2976 - val_accuracy: 0.8789\n",
            "Epoch 18/40\n",
            "30/30 [==============================] - 2s 78ms/step - loss: 0.2090 - accuracy: 0.9282 - val_loss: 0.2948 - val_accuracy: 0.8787\n",
            "Epoch 19/40\n",
            "30/30 [==============================] - 2s 78ms/step - loss: 0.1951 - accuracy: 0.9355 - val_loss: 0.2894 - val_accuracy: 0.8828\n",
            "Epoch 20/40\n",
            "30/30 [==============================] - 2s 77ms/step - loss: 0.1834 - accuracy: 0.9408 - val_loss: 0.2874 - val_accuracy: 0.8840\n",
            "Epoch 21/40\n",
            "30/30 [==============================] - 2s 75ms/step - loss: 0.1732 - accuracy: 0.9455 - val_loss: 0.2868 - val_accuracy: 0.8854\n",
            "Epoch 22/40\n",
            "30/30 [==============================] - 2s 78ms/step - loss: 0.1635 - accuracy: 0.9500 - val_loss: 0.2862 - val_accuracy: 0.8843\n",
            "Epoch 23/40\n",
            "30/30 [==============================] - 2s 77ms/step - loss: 0.1543 - accuracy: 0.9533 - val_loss: 0.2870 - val_accuracy: 0.8852\n",
            "Epoch 24/40\n",
            "30/30 [==============================] - 2s 78ms/step - loss: 0.1465 - accuracy: 0.9567 - val_loss: 0.2879 - val_accuracy: 0.8849\n",
            "Epoch 25/40\n",
            "30/30 [==============================] - 2s 77ms/step - loss: 0.1382 - accuracy: 0.9600 - val_loss: 0.2891 - val_accuracy: 0.8833\n",
            "Epoch 26/40\n",
            "30/30 [==============================] - 2s 75ms/step - loss: 0.1303 - accuracy: 0.9635 - val_loss: 0.2910 - val_accuracy: 0.8844\n",
            "Epoch 27/40\n",
            "30/30 [==============================] - 2s 75ms/step - loss: 0.1235 - accuracy: 0.9657 - val_loss: 0.2923 - val_accuracy: 0.8841\n",
            "Epoch 28/40\n",
            "30/30 [==============================] - 2s 76ms/step - loss: 0.1174 - accuracy: 0.9689 - val_loss: 0.2954 - val_accuracy: 0.8832\n",
            "Epoch 29/40\n",
            "30/30 [==============================] - 2s 78ms/step - loss: 0.1120 - accuracy: 0.9702 - val_loss: 0.2993 - val_accuracy: 0.8825\n",
            "Epoch 30/40\n",
            "30/30 [==============================] - 2s 75ms/step - loss: 0.1054 - accuracy: 0.9724 - val_loss: 0.3024 - val_accuracy: 0.8815\n",
            "Epoch 31/40\n",
            "30/30 [==============================] - 2s 75ms/step - loss: 0.1002 - accuracy: 0.9749 - val_loss: 0.3069 - val_accuracy: 0.8815\n",
            "Epoch 32/40\n",
            "30/30 [==============================] - 2s 75ms/step - loss: 0.0957 - accuracy: 0.9759 - val_loss: 0.3093 - val_accuracy: 0.8804\n",
            "Epoch 33/40\n",
            "30/30 [==============================] - 2s 77ms/step - loss: 0.0904 - accuracy: 0.9781 - val_loss: 0.3140 - val_accuracy: 0.8798\n",
            "Epoch 34/40\n",
            "30/30 [==============================] - 2s 77ms/step - loss: 0.0858 - accuracy: 0.9800 - val_loss: 0.3175 - val_accuracy: 0.8792\n",
            "Epoch 35/40\n",
            "30/30 [==============================] - 2s 74ms/step - loss: 0.0812 - accuracy: 0.9816 - val_loss: 0.3232 - val_accuracy: 0.8790\n",
            "Epoch 36/40\n",
            "30/30 [==============================] - 2s 75ms/step - loss: 0.0771 - accuracy: 0.9835 - val_loss: 0.3271 - val_accuracy: 0.8769\n",
            "Epoch 37/40\n",
            "30/30 [==============================] - 2s 74ms/step - loss: 0.0732 - accuracy: 0.9845 - val_loss: 0.3333 - val_accuracy: 0.8772\n",
            "Epoch 38/40\n",
            "30/30 [==============================] - 2s 77ms/step - loss: 0.0701 - accuracy: 0.9851 - val_loss: 0.3374 - val_accuracy: 0.8765\n",
            "Epoch 39/40\n",
            "30/30 [==============================] - 2s 77ms/step - loss: 0.0663 - accuracy: 0.9871 - val_loss: 0.3428 - val_accuracy: 0.8765\n",
            "Epoch 40/40\n",
            "30/30 [==============================] - 2s 77ms/step - loss: 0.0626 - accuracy: 0.9884 - val_loss: 0.3498 - val_accuracy: 0.8755\n"
          ]
        }
      ],
      "source": [
        "# Train and test the model\n",
        "# Train and test the model\n",
        "\n",
        "#Compile model. \n",
        "model3.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model3.fit(partial_X_train, partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1p8xesuj8Po",
        "outputId": "76dc169a-8555-4874-8fe5-0b606876a1e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 3s 4ms/step - loss: 0.3731 - accuracy: 0.8665\n"
          ]
        }
      ],
      "source": [
        "results = model3.evaluate(X_test_enc, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thUvAhqmkAie",
        "outputId": "fa3a03b7-69fc-4e31-dac1-50af004ab482"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.373094767332077, 0.8664799928665161]\n"
          ]
        }
      ],
      "source": [
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAdOh2IHnKuT"
      },
      "source": [
        "Experiments here show that fine-tuning the pre-trained embeddings would obtain better accuracy.\n",
        "\n",
        "Note that although fine-tuning the pre-trained embeddings is better here, we do not fine-tune it in other models. You could conduct extra experiments to compare the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ1KWFKvcagS"
      },
      "source": [
        "##  Model 3-2: LSTM with pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1lKisy3kb60"
      },
      "source": [
        "In previous labs, we have conducted an experiment based on LSTM. Now, we replace its embeddings with the GloVe pre-trained embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG5oTQaYkbFI",
        "outputId": "5558607d-4ec0-4689-f80a-ca1acd2f9f79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_10 (InputLayer)       [(None, 256)]             0         \n",
            "                                                                 \n",
            " GloVe_Embeddings (Embedding  (None, 256, 300)         120000300 \n",
            " )                                                               \n",
            "                                                                 \n",
            " lstm_8 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 120,160,801\n",
            "Trainable params: 120,160,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import LSTM\n",
        "# your code goes here #fine tuning.\n",
        "embeddingLayer=createPretrainedEmbeddingLayer(wordToGlove,wordToIndex,isTrainable=True)\n",
        "#Input layer. \n",
        "target_word = Input((MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "\n",
        "#Embedding layer. \n",
        "target_embedding = embeddingLayer(target_word)\n",
        "\n",
        "#Hidden layer. \n",
        "LSTM_layer = LSTM(100)(target_embedding)  \n",
        "\n",
        "#Output\n",
        "output = Dense(1,activation=\"sigmoid\")(LSTM_layer)\n",
        "\n",
        "#Initialise model \n",
        "model7 = Model(inputs=target_word, outputs=[output]) \n",
        "\n",
        "model7.summary() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqyL_rhaUElx",
        "outputId": "aca0cf36-90e5-4233-bbb4-9d4b340791d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "30/30 [==============================] - 11s 295ms/step - loss: 0.6894 - accuracy: 0.5179 - val_loss: 0.6808 - val_accuracy: 0.5364\n",
            "Epoch 2/40\n",
            "30/30 [==============================] - 8s 276ms/step - loss: 0.6640 - accuracy: 0.5771 - val_loss: 0.6770 - val_accuracy: 0.5528\n",
            "Epoch 3/40\n",
            "30/30 [==============================] - 8s 276ms/step - loss: 0.6480 - accuracy: 0.5871 - val_loss: 0.6339 - val_accuracy: 0.6107\n",
            "Epoch 4/40\n",
            "30/30 [==============================] - 8s 275ms/step - loss: 0.6695 - accuracy: 0.5876 - val_loss: 0.6696 - val_accuracy: 0.5487\n",
            "Epoch 5/40\n",
            "30/30 [==============================] - 8s 276ms/step - loss: 0.6387 - accuracy: 0.5853 - val_loss: 0.6602 - val_accuracy: 0.5584\n",
            "Epoch 6/40\n",
            "30/30 [==============================] - 8s 275ms/step - loss: 0.6085 - accuracy: 0.6122 - val_loss: 0.6875 - val_accuracy: 0.5481\n",
            "Epoch 7/40\n",
            "30/30 [==============================] - 8s 277ms/step - loss: 0.5842 - accuracy: 0.6254 - val_loss: 0.6279 - val_accuracy: 0.6208\n",
            "Epoch 8/40\n",
            "30/30 [==============================] - 8s 275ms/step - loss: 0.5605 - accuracy: 0.6644 - val_loss: 0.6394 - val_accuracy: 0.5856\n",
            "Epoch 9/40\n",
            "30/30 [==============================] - 8s 276ms/step - loss: 0.5303 - accuracy: 0.6750 - val_loss: 0.5008 - val_accuracy: 0.7940\n",
            "Epoch 10/40\n",
            "30/30 [==============================] - 8s 276ms/step - loss: 0.5175 - accuracy: 0.7640 - val_loss: 0.6970 - val_accuracy: 0.5901\n",
            "Epoch 11/40\n",
            "30/30 [==============================] - 8s 276ms/step - loss: 0.6165 - accuracy: 0.5961 - val_loss: 0.6618 - val_accuracy: 0.5596\n",
            "Epoch 12/40\n",
            "30/30 [==============================] - 8s 275ms/step - loss: 0.5598 - accuracy: 0.6325 - val_loss: 0.6462 - val_accuracy: 0.5850\n",
            "Epoch 13/40\n",
            "30/30 [==============================] - 8s 276ms/step - loss: 0.5302 - accuracy: 0.6434 - val_loss: 0.6279 - val_accuracy: 0.6053\n",
            "Epoch 14/40\n",
            "30/30 [==============================] - 8s 277ms/step - loss: 0.5405 - accuracy: 0.6700 - val_loss: 0.7570 - val_accuracy: 0.5632\n",
            "Epoch 15/40\n",
            "30/30 [==============================] - 8s 277ms/step - loss: 0.5876 - accuracy: 0.6247 - val_loss: 0.6605 - val_accuracy: 0.5657\n",
            "Epoch 16/40\n",
            "30/30 [==============================] - 8s 275ms/step - loss: 0.5348 - accuracy: 0.6457 - val_loss: 0.6483 - val_accuracy: 0.5831\n",
            "Epoch 17/40\n",
            "30/30 [==============================] - 8s 277ms/step - loss: 0.5032 - accuracy: 0.6729 - val_loss: 0.8192 - val_accuracy: 0.5400\n",
            "Epoch 18/40\n",
            "30/30 [==============================] - 8s 275ms/step - loss: 0.5317 - accuracy: 0.6851 - val_loss: 0.6202 - val_accuracy: 0.7569\n",
            "Epoch 19/40\n",
            "30/30 [==============================] - 8s 275ms/step - loss: 0.4153 - accuracy: 0.8396 - val_loss: 0.5630 - val_accuracy: 0.7525\n",
            "Epoch 20/40\n",
            "30/30 [==============================] - 8s 275ms/step - loss: 0.3779 - accuracy: 0.8515 - val_loss: 0.5460 - val_accuracy: 0.7955\n",
            "Epoch 21/40\n",
            "30/30 [==============================] - 8s 277ms/step - loss: 0.3629 - accuracy: 0.8596 - val_loss: 0.5591 - val_accuracy: 0.7694\n",
            "Epoch 22/40\n",
            "30/30 [==============================] - 8s 275ms/step - loss: 0.3495 - accuracy: 0.8627 - val_loss: 0.5498 - val_accuracy: 0.7957\n",
            "Epoch 23/40\n",
            "30/30 [==============================] - 8s 275ms/step - loss: 0.3886 - accuracy: 0.8493 - val_loss: 1.3554 - val_accuracy: 0.4988\n",
            "Epoch 24/40\n",
            "30/30 [==============================] - 8s 276ms/step - loss: 0.7949 - accuracy: 0.5311 - val_loss: 0.6857 - val_accuracy: 0.5323\n",
            "Epoch 25/40\n",
            "30/30 [==============================] - 8s 275ms/step - loss: 0.6225 - accuracy: 0.6005 - val_loss: 0.6585 - val_accuracy: 0.5567\n",
            "Epoch 26/40\n",
            "30/30 [==============================] - 8s 274ms/step - loss: 0.5723 - accuracy: 0.6355 - val_loss: 0.6261 - val_accuracy: 0.5872\n",
            "Epoch 27/40\n",
            "30/30 [==============================] - 8s 275ms/step - loss: 0.5290 - accuracy: 0.6573 - val_loss: 0.6190 - val_accuracy: 0.6172\n",
            "Epoch 28/40\n",
            "30/30 [==============================] - 8s 276ms/step - loss: 0.4779 - accuracy: 0.7293 - val_loss: 0.7145 - val_accuracy: 0.6870\n",
            "Epoch 29/40\n",
            "30/30 [==============================] - 8s 274ms/step - loss: 0.5593 - accuracy: 0.6547 - val_loss: 0.6258 - val_accuracy: 0.6112\n",
            "Epoch 30/40\n",
            "30/30 [==============================] - 8s 275ms/step - loss: 0.4913 - accuracy: 0.6737 - val_loss: 0.6307 - val_accuracy: 0.5985\n",
            "Epoch 31/40\n",
            "30/30 [==============================] - 8s 275ms/step - loss: 0.4729 - accuracy: 0.7477 - val_loss: 0.6128 - val_accuracy: 0.6911\n",
            "Epoch 32/40\n",
            "30/30 [==============================] - 8s 277ms/step - loss: 0.4738 - accuracy: 0.8216 - val_loss: 0.5314 - val_accuracy: 0.7712\n",
            "Epoch 33/40\n",
            "30/30 [==============================] - 8s 276ms/step - loss: 0.3876 - accuracy: 0.8501 - val_loss: 0.5493 - val_accuracy: 0.7713\n",
            "Epoch 34/40\n",
            "30/30 [==============================] - 8s 276ms/step - loss: 0.4699 - accuracy: 0.7615 - val_loss: 0.6138 - val_accuracy: 0.7356\n",
            "Epoch 35/40\n",
            "30/30 [==============================] - 8s 276ms/step - loss: 0.3801 - accuracy: 0.8409 - val_loss: 0.5275 - val_accuracy: 0.7952\n",
            "Epoch 36/40\n",
            "30/30 [==============================] - 8s 275ms/step - loss: 0.3598 - accuracy: 0.8659 - val_loss: 0.5180 - val_accuracy: 0.7987\n",
            "Epoch 37/40\n",
            "30/30 [==============================] - 8s 275ms/step - loss: 0.3321 - accuracy: 0.8759 - val_loss: 0.5286 - val_accuracy: 0.7928\n",
            "Epoch 38/40\n",
            "30/30 [==============================] - 8s 275ms/step - loss: 0.3396 - accuracy: 0.8679 - val_loss: 0.5545 - val_accuracy: 0.7970\n",
            "Epoch 39/40\n",
            "30/30 [==============================] - 8s 276ms/step - loss: 0.6296 - accuracy: 0.6544 - val_loss: 0.6875 - val_accuracy: 0.5580\n",
            "Epoch 40/40\n",
            "30/30 [==============================] - 8s 275ms/step - loss: 0.5688 - accuracy: 0.6351 - val_loss: 0.6662 - val_accuracy: 0.5746\n"
          ]
        }
      ],
      "source": [
        "# Train and test the model\n",
        "#Compile model. \n",
        "model7.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model7.fit(partial_X_train, partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0r82yTenT2Y",
        "outputId": "d982a428-f02c-4897-c26d-15567afa92fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 16s 21ms/step - loss: 0.6705 - accuracy: 0.5702\n"
          ]
        }
      ],
      "source": [
        "results = model7.evaluate(X_test_enc, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCR1FCofnWE2",
        "outputId": "ec113fcf-ab8a-495e-8b1a-5dd4f5321051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.6705185770988464, 0.5702000260353088]\n"
          ]
        }
      ],
      "source": [
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA-5s1h_nYAV"
      },
      "source": [
        "Freezing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06f0YSPMnhHE",
        "outputId": "31fd1f88-3549-47a2-bdcb-8ad4a3e42c84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 256)]             0         \n",
            "                                                                 \n",
            " GloVe_Embeddings (Embedding  (None, 256, 300)         120000300 \n",
            " )                                                               \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 120,160,801\n",
            "Trainable params: 160,501\n",
            "Non-trainable params: 120,000,300\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import LSTM\n",
        "# your code goes here #freezing.\n",
        "embeddingLayer=createPretrainedEmbeddingLayer(wordToGlove,wordToIndex,isTrainable=False)\n",
        "#Input layer. \n",
        "target_word = Input((MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "\n",
        "#Embedding layer. \n",
        "target_embedding = embeddingLayer(target_word)\n",
        "\n",
        "#Hidden layer. \n",
        "LSTM_layer = LSTM(100)(target_embedding)  \n",
        "\n",
        "#Output\n",
        "output = Dense(1,activation=\"sigmoid\")(LSTM_layer)\n",
        "\n",
        "#Initialise model \n",
        "model7 = Model(inputs=target_word, outputs=[output]) \n",
        "\n",
        "model7.summary() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd7En9AqnhHJ",
        "outputId": "6d295e09-6722-4c1b-fb26-84a4cbe9ca97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "30/30 [==============================] - 39s 200ms/step - loss: 0.6892 - accuracy: 0.5173 - val_loss: 0.6806 - val_accuracy: 0.5339\n",
            "Epoch 2/40\n",
            "30/30 [==============================] - 5s 180ms/step - loss: 0.6789 - accuracy: 0.5445 - val_loss: 0.6809 - val_accuracy: 0.5324\n",
            "Epoch 3/40\n",
            "30/30 [==============================] - 5s 180ms/step - loss: 0.6668 - accuracy: 0.5623 - val_loss: 0.6819 - val_accuracy: 0.5778\n",
            "Epoch 4/40\n",
            "30/30 [==============================] - 5s 179ms/step - loss: 0.6603 - accuracy: 0.5973 - val_loss: 0.6758 - val_accuracy: 0.5511\n",
            "Epoch 5/40\n",
            "30/30 [==============================] - 5s 181ms/step - loss: 0.6568 - accuracy: 0.5792 - val_loss: 0.6568 - val_accuracy: 0.5845\n",
            "Epoch 6/40\n",
            "30/30 [==============================] - 5s 180ms/step - loss: 0.6278 - accuracy: 0.6629 - val_loss: 0.6165 - val_accuracy: 0.6748\n",
            "Epoch 7/40\n",
            "30/30 [==============================] - 5s 182ms/step - loss: 0.6545 - accuracy: 0.6022 - val_loss: 0.6807 - val_accuracy: 0.5312\n",
            "Epoch 8/40\n",
            "30/30 [==============================] - 5s 180ms/step - loss: 0.6632 - accuracy: 0.5643 - val_loss: 0.6753 - val_accuracy: 0.5361\n",
            "Epoch 9/40\n",
            "30/30 [==============================] - 5s 180ms/step - loss: 0.6529 - accuracy: 0.5733 - val_loss: 0.6654 - val_accuracy: 0.5916\n",
            "Epoch 10/40\n",
            "30/30 [==============================] - 5s 179ms/step - loss: 0.6203 - accuracy: 0.6458 - val_loss: 0.6680 - val_accuracy: 0.6188\n",
            "Epoch 11/40\n",
            "30/30 [==============================] - 5s 179ms/step - loss: 0.6573 - accuracy: 0.5879 - val_loss: 0.6793 - val_accuracy: 0.5265\n",
            "Epoch 12/40\n",
            "30/30 [==============================] - 5s 178ms/step - loss: 0.6567 - accuracy: 0.5713 - val_loss: 0.6738 - val_accuracy: 0.5376\n",
            "Epoch 13/40\n",
            "30/30 [==============================] - 6s 191ms/step - loss: 0.6478 - accuracy: 0.5700 - val_loss: 0.6685 - val_accuracy: 0.5501\n",
            "Epoch 14/40\n",
            "30/30 [==============================] - 5s 180ms/step - loss: 0.6323 - accuracy: 0.5980 - val_loss: 0.6295 - val_accuracy: 0.6914\n",
            "Epoch 15/40\n",
            "30/30 [==============================] - 5s 180ms/step - loss: 0.6499 - accuracy: 0.5929 - val_loss: 0.6771 - val_accuracy: 0.5332\n",
            "Epoch 16/40\n",
            "30/30 [==============================] - 5s 178ms/step - loss: 0.6489 - accuracy: 0.5796 - val_loss: 0.6724 - val_accuracy: 0.5349\n",
            "Epoch 17/40\n",
            "30/30 [==============================] - 5s 180ms/step - loss: 0.6389 - accuracy: 0.5883 - val_loss: 0.6693 - val_accuracy: 0.5438\n",
            "Epoch 18/40\n",
            "30/30 [==============================] - 5s 178ms/step - loss: 0.6272 - accuracy: 0.5910 - val_loss: 0.6622 - val_accuracy: 0.5864\n",
            "Epoch 19/40\n",
            "30/30 [==============================] - 5s 179ms/step - loss: 0.5981 - accuracy: 0.6633 - val_loss: 0.6462 - val_accuracy: 0.6033\n",
            "Epoch 20/40\n",
            "30/30 [==============================] - 5s 180ms/step - loss: 0.6259 - accuracy: 0.6077 - val_loss: 0.6687 - val_accuracy: 0.5531\n",
            "Epoch 21/40\n",
            "30/30 [==============================] - 5s 181ms/step - loss: 0.6073 - accuracy: 0.6606 - val_loss: 0.7519 - val_accuracy: 0.5993\n",
            "Epoch 22/40\n",
            "30/30 [==============================] - 5s 180ms/step - loss: 0.6066 - accuracy: 0.6493 - val_loss: 0.6579 - val_accuracy: 0.6140\n",
            "Epoch 23/40\n",
            "30/30 [==============================] - 5s 180ms/step - loss: 0.6054 - accuracy: 0.6522 - val_loss: 0.6598 - val_accuracy: 0.5923\n",
            "Epoch 24/40\n",
            "30/30 [==============================] - 5s 179ms/step - loss: 0.6394 - accuracy: 0.6017 - val_loss: 0.6681 - val_accuracy: 0.5853\n",
            "Epoch 25/40\n",
            "30/30 [==============================] - 5s 180ms/step - loss: 0.6002 - accuracy: 0.6799 - val_loss: 0.6811 - val_accuracy: 0.6043\n",
            "Epoch 26/40\n",
            "30/30 [==============================] - 5s 179ms/step - loss: 0.6021 - accuracy: 0.6837 - val_loss: 0.6408 - val_accuracy: 0.6536\n",
            "Epoch 27/40\n",
            "30/30 [==============================] - 5s 180ms/step - loss: 0.6026 - accuracy: 0.6645 - val_loss: 0.6483 - val_accuracy: 0.6285\n",
            "Epoch 28/40\n",
            "30/30 [==============================] - 5s 180ms/step - loss: 0.6156 - accuracy: 0.6218 - val_loss: 0.6580 - val_accuracy: 0.5829\n",
            "Epoch 29/40\n",
            "30/30 [==============================] - 5s 180ms/step - loss: 0.5974 - accuracy: 0.6695 - val_loss: 0.6447 - val_accuracy: 0.6285\n",
            "Epoch 30/40\n",
            "30/30 [==============================] - 5s 180ms/step - loss: 0.5856 - accuracy: 0.6731 - val_loss: 0.6447 - val_accuracy: 0.6195\n",
            "Epoch 31/40\n",
            "30/30 [==============================] - 5s 180ms/step - loss: 0.5891 - accuracy: 0.6458 - val_loss: 0.6574 - val_accuracy: 0.5841\n",
            "Epoch 32/40\n",
            "30/30 [==============================] - 5s 179ms/step - loss: 0.5757 - accuracy: 0.6745 - val_loss: 0.6392 - val_accuracy: 0.6608\n",
            "Epoch 33/40\n",
            "30/30 [==============================] - 5s 180ms/step - loss: 0.5527 - accuracy: 0.7144 - val_loss: 0.6271 - val_accuracy: 0.6736\n",
            "Epoch 34/40\n",
            "30/30 [==============================] - 5s 179ms/step - loss: 0.5532 - accuracy: 0.7281 - val_loss: 0.6521 - val_accuracy: 0.6695\n",
            "Epoch 35/40\n",
            "30/30 [==============================] - 5s 180ms/step - loss: 0.6069 - accuracy: 0.6645 - val_loss: 0.7195 - val_accuracy: 0.5169\n",
            "Epoch 36/40\n",
            "30/30 [==============================] - 5s 179ms/step - loss: 0.6398 - accuracy: 0.5851 - val_loss: 0.6828 - val_accuracy: 0.5329\n",
            "Epoch 37/40\n",
            "30/30 [==============================] - 5s 181ms/step - loss: 0.6176 - accuracy: 0.6055 - val_loss: 0.6846 - val_accuracy: 0.5343\n",
            "Epoch 38/40\n",
            "30/30 [==============================] - 5s 180ms/step - loss: 0.6073 - accuracy: 0.6111 - val_loss: 0.6857 - val_accuracy: 0.5346\n",
            "Epoch 39/40\n",
            "30/30 [==============================] - 5s 181ms/step - loss: 0.5987 - accuracy: 0.6135 - val_loss: 0.6883 - val_accuracy: 0.5368\n",
            "Epoch 40/40\n",
            "30/30 [==============================] - 5s 179ms/step - loss: 0.5905 - accuracy: 0.6263 - val_loss: 0.6879 - val_accuracy: 0.5839\n"
          ]
        }
      ],
      "source": [
        "# Train and test the model\n",
        "#Compile model. \n",
        "model7.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model7.fit(partial_X_train, partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE3w-0sRnhHK",
        "outputId": "982ed15a-5ba6-43e8-a2c4-7925e5d1510b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 17s 21ms/step - loss: 0.6864 - accuracy: 0.5901\n"
          ]
        }
      ],
      "source": [
        "results = model7.evaluate(X_test_enc, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0nsGl8pnhHK",
        "outputId": "bc50f93e-4ea3-4e12-eae4-6efbbd1f11a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.6863669753074646, 0.5901200175285339]\n"
          ]
        }
      ],
      "source": [
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOqePi5-I1Xq"
      },
      "source": [
        "Experiment show that simply replacing the lab 2 model embeddings with pre-trained word embeddings (GloVe) will cause performance to drop significantly. \n",
        "What can you do to improve the situation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-bZ5SCHiIMl"
      },
      "source": [
        "#  Model 4: Adding extra dense layer into Neural averaging network model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G85QM3lSV7qp"
      },
      "source": [
        "We add extra dense layers into model 3-1 (neural network model) to evaluate extra dense layers' contribution. We start by adding one layer, then add two. All parameters are the same as model 3-1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExgX8bxpVgps"
      },
      "source": [
        "### Adding one extra dense layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTgD_gMzXa1z",
        "outputId": "d594f050-eaaa-4481-e568-9a9778c431bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_11 (InputLayer)       [(None, 256)]             0         \n",
            "                                                                 \n",
            " GloVe_Embeddings (Embedding  (None, 256, 300)         120000300 \n",
            " )                                                               \n",
            "                                                                 \n",
            " global_average_pooling1d_ma  (None, 300)              0         \n",
            " sked_1 (GlobalAveragePoolin                                     \n",
            " g1DMasked)                                                      \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 100)               30100     \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 16)                1616      \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 120,032,033\n",
            "Trainable params: 120,032,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Inspect model performance with 1 hidden layer, pre-trained embeddings\n",
        "# your code goes here\n",
        "#Input layer. \n",
        "target_word = Input((MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "\n",
        "#Embedding layer. \n",
        "target_embedding = embeddingLayer(target_word)\n",
        "\n",
        "#Average one hot vectors. \n",
        "global_pol = GlobalAveragePooling1DMasked()(target_embedding)\n",
        "\n",
        "#Extra dense layer. \n",
        "extra_dense_layer = Dense(100, activation=\"sigmoid\")(global_pol)  \n",
        "\n",
        "#Hidden layer. \n",
        "hidden_layer = Dense(16, activation=\"sigmoid\")(extra_dense_layer) \n",
        "#Output\n",
        "output = Dense(1,activation=\"sigmoid\")(hidden_layer)\n",
        "\n",
        "#Initialise model \n",
        "model7 = Model(inputs=target_word, outputs=[output]) \n",
        "\n",
        "model7.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAQtKxqdqAVu",
        "outputId": "f862cc92-d334-4581-f01d-9ff47f205030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "30/30 [==============================] - 4s 124ms/step - loss: 0.7026 - accuracy: 0.5053 - val_loss: 0.6876 - val_accuracy: 0.5870\n",
            "Epoch 2/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.6826 - accuracy: 0.6415 - val_loss: 0.6770 - val_accuracy: 0.7443\n",
            "Epoch 3/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.6647 - accuracy: 0.7619 - val_loss: 0.6527 - val_accuracy: 0.7635\n",
            "Epoch 4/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.6244 - accuracy: 0.7713 - val_loss: 0.5982 - val_accuracy: 0.7728\n",
            "Epoch 5/40\n",
            "30/30 [==============================] - 3s 117ms/step - loss: 0.5465 - accuracy: 0.8043 - val_loss: 0.5107 - val_accuracy: 0.8109\n",
            "Epoch 6/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.4464 - accuracy: 0.8459 - val_loss: 0.4220 - val_accuracy: 0.8439\n",
            "Epoch 7/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.3618 - accuracy: 0.8769 - val_loss: 0.3649 - val_accuracy: 0.8599\n",
            "Epoch 8/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.3078 - accuracy: 0.8911 - val_loss: 0.3327 - val_accuracy: 0.8706\n",
            "Epoch 9/40\n",
            "30/30 [==============================] - 3s 117ms/step - loss: 0.2712 - accuracy: 0.9036 - val_loss: 0.3142 - val_accuracy: 0.8752\n",
            "Epoch 10/40\n",
            "30/30 [==============================] - 3s 117ms/step - loss: 0.2431 - accuracy: 0.9137 - val_loss: 0.3029 - val_accuracy: 0.8799\n",
            "Epoch 11/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.2212 - accuracy: 0.9249 - val_loss: 0.2959 - val_accuracy: 0.8829\n",
            "Epoch 12/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.2003 - accuracy: 0.9329 - val_loss: 0.2932 - val_accuracy: 0.8805\n",
            "Epoch 13/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.1815 - accuracy: 0.9404 - val_loss: 0.2898 - val_accuracy: 0.8855\n",
            "Epoch 14/40\n",
            "30/30 [==============================] - 3s 117ms/step - loss: 0.1654 - accuracy: 0.9488 - val_loss: 0.2892 - val_accuracy: 0.8848\n",
            "Epoch 15/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.1518 - accuracy: 0.9529 - val_loss: 0.2910 - val_accuracy: 0.8850\n",
            "Epoch 16/40\n",
            "30/30 [==============================] - 3s 117ms/step - loss: 0.1392 - accuracy: 0.9585 - val_loss: 0.2982 - val_accuracy: 0.8839\n",
            "Epoch 17/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.1289 - accuracy: 0.9622 - val_loss: 0.3043 - val_accuracy: 0.8809\n",
            "Epoch 18/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.1189 - accuracy: 0.9661 - val_loss: 0.3042 - val_accuracy: 0.8834\n",
            "Epoch 19/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.1063 - accuracy: 0.9720 - val_loss: 0.3114 - val_accuracy: 0.8823\n",
            "Epoch 20/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0974 - accuracy: 0.9749 - val_loss: 0.3153 - val_accuracy: 0.8815\n",
            "Epoch 21/40\n",
            "30/30 [==============================] - 3s 117ms/step - loss: 0.0898 - accuracy: 0.9783 - val_loss: 0.3229 - val_accuracy: 0.8815\n",
            "Epoch 22/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0823 - accuracy: 0.9809 - val_loss: 0.3317 - val_accuracy: 0.8813\n",
            "Epoch 23/40\n",
            "30/30 [==============================] - 3s 117ms/step - loss: 0.0763 - accuracy: 0.9834 - val_loss: 0.3400 - val_accuracy: 0.8813\n",
            "Epoch 24/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0683 - accuracy: 0.9861 - val_loss: 0.3484 - val_accuracy: 0.8806\n",
            "Epoch 25/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0631 - accuracy: 0.9885 - val_loss: 0.3540 - val_accuracy: 0.8792\n",
            "Epoch 26/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0582 - accuracy: 0.9898 - val_loss: 0.3633 - val_accuracy: 0.8794\n",
            "Epoch 27/40\n",
            "30/30 [==============================] - 3s 117ms/step - loss: 0.0541 - accuracy: 0.9907 - val_loss: 0.3724 - val_accuracy: 0.8777\n",
            "Epoch 28/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0501 - accuracy: 0.9919 - val_loss: 0.3828 - val_accuracy: 0.8765\n",
            "Epoch 29/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0476 - accuracy: 0.9925 - val_loss: 0.3891 - val_accuracy: 0.8754\n",
            "Epoch 30/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0433 - accuracy: 0.9939 - val_loss: 0.3989 - val_accuracy: 0.8743\n",
            "Epoch 31/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0410 - accuracy: 0.9941 - val_loss: 0.4091 - val_accuracy: 0.8745\n",
            "Epoch 32/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0384 - accuracy: 0.9947 - val_loss: 0.4168 - val_accuracy: 0.8726\n",
            "Epoch 33/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0362 - accuracy: 0.9953 - val_loss: 0.4265 - val_accuracy: 0.8719\n",
            "Epoch 34/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0341 - accuracy: 0.9955 - val_loss: 0.4325 - val_accuracy: 0.8719\n",
            "Epoch 35/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0325 - accuracy: 0.9959 - val_loss: 0.4395 - val_accuracy: 0.8709\n",
            "Epoch 36/40\n",
            "30/30 [==============================] - 3s 117ms/step - loss: 0.0311 - accuracy: 0.9963 - val_loss: 0.4506 - val_accuracy: 0.8697\n",
            "Epoch 37/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0301 - accuracy: 0.9963 - val_loss: 0.4549 - val_accuracy: 0.8700\n",
            "Epoch 38/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0287 - accuracy: 0.9963 - val_loss: 0.4625 - val_accuracy: 0.8698\n",
            "Epoch 39/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0279 - accuracy: 0.9966 - val_loss: 0.4659 - val_accuracy: 0.8714\n",
            "Epoch 40/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0267 - accuracy: 0.9965 - val_loss: 0.4724 - val_accuracy: 0.8717\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5130 - accuracy: 0.8572\n",
            "[0.5129744410514832, 0.8572400212287903]\n"
          ]
        }
      ],
      "source": [
        "# Train and test the model\n",
        "#Compile model. \n",
        "model7.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model7.fit(partial_X_train, partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)\n",
        "results = model7.evaluate(X_test_enc, y_test)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0npTvFuVt5R"
      },
      "source": [
        "### Adding two extra dense layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PowyhyRqcdDA",
        "outputId": "0da5b6ad-f3d5-4485-fc7c-7340a050e624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_13 (InputLayer)       [(None, 256)]             0         \n",
            "                                                                 \n",
            " GloVe_Embeddings (Embedding  (None, 256, 300)         120000300 \n",
            " )                                                               \n",
            "                                                                 \n",
            " global_average_pooling1d_ma  (None, 300)              0         \n",
            " sked_3 (GlobalAveragePoolin                                     \n",
            " g1DMasked)                                                      \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 300)               90300     \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 100)               30100     \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 16)                1616      \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 120,122,333\n",
            "Trainable params: 120,122,333\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Inspect model performance with 1 hidden layer, pre-trained embeddings\n",
        "# your code goes here\n",
        "#Input layer. \n",
        "target_word = Input((MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "\n",
        "#Embedding layer. \n",
        "target_embedding = embeddingLayer(target_word)\n",
        "\n",
        "#Average one hot vectors. \n",
        "global_pol = GlobalAveragePooling1DMasked()(target_embedding)\n",
        "\n",
        "#Extra dense layer 1. \n",
        "extra_dense_1 = Dense(300, activation=\"sigmoid\")(global_pol)  \n",
        "\n",
        "#Extra dense layer 2. \n",
        "extra_dense_2 = Dense(100, activation=\"sigmoid\")(extra_dense_1)\n",
        "\n",
        "#Hidden layer. \n",
        "hidden_layer = Dense(16, activation=\"sigmoid\")(extra_dense_2)  #Check between tahn, relu... keep the best.\n",
        "\n",
        "#Output\n",
        "output = Dense(1,activation=\"sigmoid\")(hidden_layer)\n",
        "\n",
        "#Initialise model \n",
        "model7 = Model(inputs=target_word, outputs=[output]) \n",
        "\n",
        "model7.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOasVi7XqUM-",
        "outputId": "e3c8308a-51c9-42a3-e3a3-21a83785ff89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "30/30 [==============================] - 4s 124ms/step - loss: 0.6834 - accuracy: 0.6014 - val_loss: 0.6661 - val_accuracy: 0.6611\n",
            "Epoch 2/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.5753 - accuracy: 0.8944 - val_loss: 0.4742 - val_accuracy: 0.8584\n",
            "Epoch 3/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.3001 - accuracy: 0.9641 - val_loss: 0.3358 - val_accuracy: 0.8732\n",
            "Epoch 4/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.1585 - accuracy: 0.9893 - val_loss: 0.3231 - val_accuracy: 0.8750\n",
            "Epoch 5/40\n",
            "30/30 [==============================] - 3s 117ms/step - loss: 0.1051 - accuracy: 0.9958 - val_loss: 0.3315 - val_accuracy: 0.8740\n",
            "Epoch 6/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0790 - accuracy: 0.9972 - val_loss: 0.3453 - val_accuracy: 0.8736\n",
            "Epoch 7/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0639 - accuracy: 0.9975 - val_loss: 0.3594 - val_accuracy: 0.8725\n",
            "Epoch 8/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0539 - accuracy: 0.9976 - val_loss: 0.3731 - val_accuracy: 0.8715\n",
            "Epoch 9/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0472 - accuracy: 0.9977 - val_loss: 0.3853 - val_accuracy: 0.8707\n",
            "Epoch 10/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0420 - accuracy: 0.9977 - val_loss: 0.3992 - val_accuracy: 0.8723\n",
            "Epoch 11/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0382 - accuracy: 0.9975 - val_loss: 0.4086 - val_accuracy: 0.8715\n",
            "Epoch 12/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0350 - accuracy: 0.9977 - val_loss: 0.4188 - val_accuracy: 0.8699\n",
            "Epoch 13/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0325 - accuracy: 0.9977 - val_loss: 0.4329 - val_accuracy: 0.8707\n",
            "Epoch 14/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0304 - accuracy: 0.9977 - val_loss: 0.4390 - val_accuracy: 0.8707\n",
            "Epoch 15/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0287 - accuracy: 0.9977 - val_loss: 0.4500 - val_accuracy: 0.8709\n",
            "Epoch 16/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0273 - accuracy: 0.9977 - val_loss: 0.4565 - val_accuracy: 0.8697\n",
            "Epoch 17/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0260 - accuracy: 0.9977 - val_loss: 0.4658 - val_accuracy: 0.8715\n",
            "Epoch 18/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0249 - accuracy: 0.9977 - val_loss: 0.4745 - val_accuracy: 0.8713\n",
            "Epoch 19/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0240 - accuracy: 0.9977 - val_loss: 0.4812 - val_accuracy: 0.8709\n",
            "Epoch 20/40\n",
            "30/30 [==============================] - 3s 117ms/step - loss: 0.0232 - accuracy: 0.9977 - val_loss: 0.4892 - val_accuracy: 0.8707\n",
            "Epoch 21/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0225 - accuracy: 0.9977 - val_loss: 0.4963 - val_accuracy: 0.8714\n",
            "Epoch 22/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0219 - accuracy: 0.9977 - val_loss: 0.5022 - val_accuracy: 0.8703\n",
            "Epoch 23/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0213 - accuracy: 0.9977 - val_loss: 0.5087 - val_accuracy: 0.8707\n",
            "Epoch 24/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0208 - accuracy: 0.9977 - val_loss: 0.5150 - val_accuracy: 0.8705\n",
            "Epoch 25/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0204 - accuracy: 0.9977 - val_loss: 0.5223 - val_accuracy: 0.8708\n",
            "Epoch 26/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0200 - accuracy: 0.9977 - val_loss: 0.5275 - val_accuracy: 0.8705\n",
            "Epoch 27/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0197 - accuracy: 0.9977 - val_loss: 0.5323 - val_accuracy: 0.8699\n",
            "Epoch 28/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0193 - accuracy: 0.9977 - val_loss: 0.5377 - val_accuracy: 0.8698\n",
            "Epoch 29/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0191 - accuracy: 0.9977 - val_loss: 0.5428 - val_accuracy: 0.8698\n",
            "Epoch 30/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0188 - accuracy: 0.9977 - val_loss: 0.5483 - val_accuracy: 0.8700\n",
            "Epoch 31/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0186 - accuracy: 0.9977 - val_loss: 0.5534 - val_accuracy: 0.8698\n",
            "Epoch 32/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0184 - accuracy: 0.9977 - val_loss: 0.5577 - val_accuracy: 0.8700\n",
            "Epoch 33/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0182 - accuracy: 0.9977 - val_loss: 0.5628 - val_accuracy: 0.8702\n",
            "Epoch 34/40\n",
            "30/30 [==============================] - 4s 118ms/step - loss: 0.0180 - accuracy: 0.9977 - val_loss: 0.5671 - val_accuracy: 0.8700\n",
            "Epoch 35/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0178 - accuracy: 0.9977 - val_loss: 0.5724 - val_accuracy: 0.8700\n",
            "Epoch 36/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0177 - accuracy: 0.9977 - val_loss: 0.5758 - val_accuracy: 0.8697\n",
            "Epoch 37/40\n",
            "30/30 [==============================] - 3s 117ms/step - loss: 0.0175 - accuracy: 0.9977 - val_loss: 0.5800 - val_accuracy: 0.8702\n",
            "Epoch 38/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0174 - accuracy: 0.9977 - val_loss: 0.5842 - val_accuracy: 0.8702\n",
            "Epoch 39/40\n",
            "30/30 [==============================] - 4s 117ms/step - loss: 0.0173 - accuracy: 0.9977 - val_loss: 0.5887 - val_accuracy: 0.8700\n",
            "Epoch 40/40\n",
            "30/30 [==============================] - 3s 116ms/step - loss: 0.0172 - accuracy: 0.9977 - val_loss: 0.5921 - val_accuracy: 0.8701\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6366 - accuracy: 0.8572\n",
            "[0.6366241574287415, 0.8571599721908569]\n"
          ]
        }
      ],
      "source": [
        "model7.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model7.fit(partial_X_train, partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)\n",
        "results = model7.evaluate(X_test_enc, y_test)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XisS32PaTATf"
      },
      "source": [
        "These two experiments show that adding extra dense layers can slightly improve accuracy over model 3-1.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrfC9Mu-RHID"
      },
      "source": [
        "#  Model 5: CNN for Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPaFmFk1BMFf"
      },
      "source": [
        "In this section, we build a Convolutional Neural Network (CNN) for text classification. We start by using one CNN layer and then adding another layer. You could use embeddings from pre-trained or scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjOJrGCzwGM5"
      },
      "source": [
        "##  Model 5-1: Basic CNN model for Text Classification "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXBstOw0sCYF",
        "outputId": "eee6ce61-672f-4f33-9c7c-b587aa20b7c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_14 (InputLayer)       [(None, 256)]             0         \n",
            "                                                                 \n",
            " target_embed_layer (Embeddi  (None, 256, 300)         3000000   \n",
            " ng)                                                             \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 251, 100)          180100    \n",
            "                                                                 \n",
            " global_average_pooling1d_ma  (None, 100)              0         \n",
            " sked_4 (GlobalAveragePoolin                                     \n",
            " g1DMasked)                                                      \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,180,201\n",
            "Trainable params: 3,180,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "hidden_layer=16\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "# your code goes here\n",
        "#Input layer. \n",
        "target_word = Input((MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "\n",
        "#Embedding layer. \n",
        "target_embedding = Embedding(VOCAB_SIZE, EMBED_SIZE, name='target_embed_layer',\n",
        "                        \tembeddings_initializer='glorot_uniform',\n",
        "                         \tinput_length=1)(target_word)\n",
        "                          \n",
        "#Convolutional layer\n",
        "CNN1D = Conv1D(100, kernel_size=6)(target_embedding)\n",
        "\n",
        "#Max pooling \n",
        "MaxPool=GlobalAveragePooling1DMasked()(CNN1D)\n",
        "\n",
        "#Output\n",
        "output = Dense(1,activation=\"sigmoid\")(MaxPool)\n",
        "\n",
        "#Initialise model \n",
        "model4 = Model(inputs=target_word, outputs=[output]) \n",
        "\n",
        "model4.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G5MrDPJFBzK",
        "outputId": "84966e7c-53b7-458a-f86c-f41fa461e84f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "30/30 [==============================] - 12s 174ms/step - loss: 0.6447 - accuracy: 0.6385 - val_loss: 0.5295 - val_accuracy: 0.7558\n",
            "Epoch 2/40\n",
            "30/30 [==============================] - 4s 130ms/step - loss: 0.3796 - accuracy: 0.8493 - val_loss: 0.3263 - val_accuracy: 0.8672\n",
            "Epoch 3/40\n",
            "30/30 [==============================] - 4s 131ms/step - loss: 0.2357 - accuracy: 0.9075 - val_loss: 0.2949 - val_accuracy: 0.8839\n",
            "Epoch 4/40\n",
            "30/30 [==============================] - 4s 131ms/step - loss: 0.1631 - accuracy: 0.9430 - val_loss: 0.3033 - val_accuracy: 0.8826\n",
            "Epoch 5/40\n",
            "30/30 [==============================] - 4s 131ms/step - loss: 0.1201 - accuracy: 0.9601 - val_loss: 0.3419 - val_accuracy: 0.8762\n",
            "Epoch 6/40\n",
            "30/30 [==============================] - 4s 131ms/step - loss: 0.0896 - accuracy: 0.9737 - val_loss: 0.3943 - val_accuracy: 0.8703\n",
            "Epoch 7/40\n",
            "30/30 [==============================] - 4s 131ms/step - loss: 0.0649 - accuracy: 0.9819 - val_loss: 0.4447 - val_accuracy: 0.8692\n",
            "Epoch 8/40\n",
            "30/30 [==============================] - 4s 131ms/step - loss: 0.0453 - accuracy: 0.9896 - val_loss: 0.4965 - val_accuracy: 0.8679\n",
            "Epoch 9/40\n",
            "30/30 [==============================] - 4s 130ms/step - loss: 0.0299 - accuracy: 0.9948 - val_loss: 0.5564 - val_accuracy: 0.8629\n",
            "Epoch 10/40\n",
            "30/30 [==============================] - 4s 130ms/step - loss: 0.0210 - accuracy: 0.9970 - val_loss: 0.6136 - val_accuracy: 0.8618\n",
            "Epoch 11/40\n",
            "30/30 [==============================] - 4s 130ms/step - loss: 0.0151 - accuracy: 0.9983 - val_loss: 0.6726 - val_accuracy: 0.8616\n",
            "Epoch 12/40\n",
            "30/30 [==============================] - 4s 129ms/step - loss: 0.0116 - accuracy: 0.9987 - val_loss: 0.7266 - val_accuracy: 0.8586\n",
            "Epoch 13/40\n",
            "30/30 [==============================] - 4s 129ms/step - loss: 0.0088 - accuracy: 0.9991 - val_loss: 0.7745 - val_accuracy: 0.8592\n",
            "Epoch 14/40\n",
            "30/30 [==============================] - 4s 130ms/step - loss: 0.0066 - accuracy: 0.9997 - val_loss: 0.8105 - val_accuracy: 0.8601\n",
            "Epoch 15/40\n",
            "30/30 [==============================] - 4s 130ms/step - loss: 0.0049 - accuracy: 0.9998 - val_loss: 0.8498 - val_accuracy: 0.8589\n",
            "Epoch 16/40\n",
            "30/30 [==============================] - 4s 130ms/step - loss: 0.0037 - accuracy: 0.9999 - val_loss: 0.8917 - val_accuracy: 0.8574\n",
            "Epoch 17/40\n",
            "30/30 [==============================] - 4s 130ms/step - loss: 0.0028 - accuracy: 0.9999 - val_loss: 0.9098 - val_accuracy: 0.8587\n",
            "Epoch 18/40\n",
            "30/30 [==============================] - 4s 129ms/step - loss: 0.0023 - accuracy: 0.9999 - val_loss: 0.9357 - val_accuracy: 0.8577\n",
            "Epoch 19/40\n",
            "30/30 [==============================] - 4s 129ms/step - loss: 0.0021 - accuracy: 0.9999 - val_loss: 0.9613 - val_accuracy: 0.8566\n",
            "Epoch 20/40\n",
            "30/30 [==============================] - 4s 129ms/step - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.9828 - val_accuracy: 0.8577\n",
            "Epoch 21/40\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 1.0067 - val_accuracy: 0.8552\n",
            "Epoch 22/40\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 1.0248 - val_accuracy: 0.8555\n",
            "Epoch 23/40\n",
            "30/30 [==============================] - 4s 129ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 1.0435 - val_accuracy: 0.8552\n",
            "Epoch 24/40\n",
            "30/30 [==============================] - 4s 129ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 1.0668 - val_accuracy: 0.8553\n",
            "Epoch 25/40\n",
            "30/30 [==============================] - 4s 129ms/step - loss: 9.1243e-04 - accuracy: 1.0000 - val_loss: 1.0805 - val_accuracy: 0.8553\n",
            "Epoch 26/40\n",
            "30/30 [==============================] - 4s 134ms/step - loss: 8.0159e-04 - accuracy: 0.9999 - val_loss: 1.0963 - val_accuracy: 0.8554\n",
            "Epoch 27/40\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 7.2410e-04 - accuracy: 1.0000 - val_loss: 1.1083 - val_accuracy: 0.8563\n",
            "Epoch 28/40\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 7.4043e-04 - accuracy: 0.9999 - val_loss: 1.1241 - val_accuracy: 0.8545\n",
            "Epoch 29/40\n",
            "30/30 [==============================] - 4s 134ms/step - loss: 6.1910e-04 - accuracy: 1.0000 - val_loss: 1.1392 - val_accuracy: 0.8563\n",
            "Epoch 30/40\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 6.2356e-04 - accuracy: 1.0000 - val_loss: 1.1531 - val_accuracy: 0.8553\n",
            "Epoch 31/40\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 6.2919e-04 - accuracy: 0.9999 - val_loss: 1.1750 - val_accuracy: 0.8538\n",
            "Epoch 32/40\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 5.2497e-04 - accuracy: 1.0000 - val_loss: 1.1801 - val_accuracy: 0.8552\n",
            "Epoch 33/40\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 4.8428e-04 - accuracy: 1.0000 - val_loss: 1.1948 - val_accuracy: 0.8543\n",
            "Epoch 34/40\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 4.3612e-04 - accuracy: 1.0000 - val_loss: 1.2078 - val_accuracy: 0.8542\n",
            "Epoch 35/40\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 3.7587e-04 - accuracy: 1.0000 - val_loss: 1.2164 - val_accuracy: 0.8552\n",
            "Epoch 36/40\n",
            "30/30 [==============================] - 4s 129ms/step - loss: 3.5803e-04 - accuracy: 1.0000 - val_loss: 1.2289 - val_accuracy: 0.8539\n",
            "Epoch 37/40\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 3.2641e-04 - accuracy: 1.0000 - val_loss: 1.2387 - val_accuracy: 0.8536\n",
            "Epoch 38/40\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 3.0504e-04 - accuracy: 1.0000 - val_loss: 1.2475 - val_accuracy: 0.8547\n",
            "Epoch 39/40\n",
            "30/30 [==============================] - 4s 128ms/step - loss: 2.8947e-04 - accuracy: 1.0000 - val_loss: 1.2601 - val_accuracy: 0.8536\n",
            "Epoch 40/40\n",
            "30/30 [==============================] - 4s 133ms/step - loss: 2.7056e-04 - accuracy: 1.0000 - val_loss: 1.2683 - val_accuracy: 0.8535\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 1.3594 - accuracy: 0.8417\n",
            "[1.3593778610229492, 0.8416799902915955]\n"
          ]
        }
      ],
      "source": [
        "# Train and test the model\n",
        "\n",
        "# your code goes here\n",
        "model4.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model4.fit(partial_X_train, partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)\n",
        "\n",
        "results = model4.evaluate(X_test_enc, y_test)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "gkMat0XJtLNk",
        "outputId": "b788a513-9d19-43fc-e655-d672f9f39708"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# your code goes here\n",
        "plt.plot(history.history['loss'], linestyle=\"\",marker=\"o\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkQMLxw-wWG6"
      },
      "source": [
        "## Model 5-2: Adding extra convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7w-y_uobvHuE",
        "outputId": "52d10d07-5814-454b-9a2d-a1a16965c920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_15 (InputLayer)       [(None, 256)]             0         \n",
            "                                                                 \n",
            " target_embed_layer (Embeddi  (None, 256, 300)         3000000   \n",
            " ng)                                                             \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 251, 100)          180100    \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 246, 100)          60100     \n",
            "                                                                 \n",
            " global_average_pooling1d_ma  (None, 100)              0         \n",
            " sked_5 (GlobalAveragePoolin                                     \n",
            " g1DMasked)                                                      \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,240,301\n",
            "Trainable params: 3,240,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#Input layer. \n",
        "target_word = Input((MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "\n",
        "#Embedding layer. \n",
        "target_embedding = Embedding(VOCAB_SIZE, EMBED_SIZE, name='target_embed_layer',\n",
        "                        \tembeddings_initializer='glorot_uniform',\n",
        "                         \tinput_length=1)(target_word)\n",
        "                          \n",
        "#Convolutional layer\n",
        "CNN1D = Conv1D(100, kernel_size=6)(target_embedding)\n",
        "\n",
        "#Convolutional layer\n",
        "CNN2D = Conv1D(100, kernel_size=6)(CNN1D)\n",
        "\n",
        "#Max pooling \n",
        "MaxPool=GlobalAveragePooling1DMasked()(CNN2D)\n",
        "\n",
        "#Output\n",
        "output = Dense(1,activation=\"sigmoid\")(MaxPool)\n",
        "\n",
        "#Initialise model \n",
        "model6 = Model(inputs=target_word, outputs=[output]) \n",
        "\n",
        "model6.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx8JLl4xPlua",
        "outputId": "1dd4fcfb-f49e-4300-f4a8-9ce9e098ea99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "30/30 [==============================] - 7s 186ms/step - loss: 0.5712 - accuracy: 0.6813 - val_loss: 0.3449 - val_accuracy: 0.8570\n",
            "Epoch 2/40\n",
            "30/30 [==============================] - 5s 181ms/step - loss: 0.2609 - accuracy: 0.8963 - val_loss: 0.3049 - val_accuracy: 0.8748\n",
            "Epoch 3/40\n",
            "30/30 [==============================] - 5s 178ms/step - loss: 0.1579 - accuracy: 0.9429 - val_loss: 0.3649 - val_accuracy: 0.8754\n",
            "Epoch 4/40\n",
            "30/30 [==============================] - 5s 180ms/step - loss: 0.0975 - accuracy: 0.9654 - val_loss: 0.5076 - val_accuracy: 0.8588\n",
            "Epoch 5/40\n",
            "30/30 [==============================] - 5s 171ms/step - loss: 0.0630 - accuracy: 0.9789 - val_loss: 0.6777 - val_accuracy: 0.8561\n",
            "Epoch 6/40\n",
            "30/30 [==============================] - 5s 171ms/step - loss: 0.0445 - accuracy: 0.9833 - val_loss: 0.8602 - val_accuracy: 0.8460\n",
            "Epoch 7/40\n",
            "30/30 [==============================] - 5s 171ms/step - loss: 0.0338 - accuracy: 0.9867 - val_loss: 0.9355 - val_accuracy: 0.8564\n",
            "Epoch 8/40\n",
            "30/30 [==============================] - 5s 171ms/step - loss: 0.0205 - accuracy: 0.9928 - val_loss: 1.1029 - val_accuracy: 0.8531\n",
            "Epoch 9/40\n",
            "30/30 [==============================] - 5s 170ms/step - loss: 0.0166 - accuracy: 0.9938 - val_loss: 1.2701 - val_accuracy: 0.8507\n",
            "Epoch 10/40\n",
            "30/30 [==============================] - 5s 169ms/step - loss: 0.0190 - accuracy: 0.9931 - val_loss: 1.3594 - val_accuracy: 0.8494\n",
            "Epoch 11/40\n",
            "30/30 [==============================] - 5s 169ms/step - loss: 0.0186 - accuracy: 0.9928 - val_loss: 1.4359 - val_accuracy: 0.8464\n",
            "Epoch 12/40\n",
            "30/30 [==============================] - 5s 169ms/step - loss: 0.0175 - accuracy: 0.9933 - val_loss: 1.5156 - val_accuracy: 0.8482\n",
            "Epoch 13/40\n",
            "30/30 [==============================] - 5s 169ms/step - loss: 0.0248 - accuracy: 0.9904 - val_loss: 1.5558 - val_accuracy: 0.8432\n",
            "Epoch 14/40\n",
            "30/30 [==============================] - 5s 169ms/step - loss: 0.0275 - accuracy: 0.9888 - val_loss: 1.4029 - val_accuracy: 0.8490\n",
            "Epoch 15/40\n",
            "30/30 [==============================] - 5s 169ms/step - loss: 0.0162 - accuracy: 0.9937 - val_loss: 1.4539 - val_accuracy: 0.8464\n",
            "Epoch 16/40\n",
            "30/30 [==============================] - 5s 169ms/step - loss: 0.0090 - accuracy: 0.9973 - val_loss: 1.6133 - val_accuracy: 0.8448\n",
            "Epoch 17/40\n",
            "30/30 [==============================] - 5s 170ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 1.8322 - val_accuracy: 0.8447\n",
            "Epoch 18/40\n",
            "30/30 [==============================] - 5s 169ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 1.9665 - val_accuracy: 0.8450\n",
            "Epoch 19/40\n",
            "30/30 [==============================] - 6s 189ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 2.0964 - val_accuracy: 0.8449\n",
            "Epoch 20/40\n",
            "30/30 [==============================] - 5s 167ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 2.1445 - val_accuracy: 0.8465\n",
            "Epoch 21/40\n",
            "30/30 [==============================] - 5s 167ms/step - loss: 6.2187e-04 - accuracy: 0.9998 - val_loss: 2.2223 - val_accuracy: 0.8485\n",
            "Epoch 22/40\n",
            "30/30 [==============================] - 5s 167ms/step - loss: 2.7069e-04 - accuracy: 0.9999 - val_loss: 2.2811 - val_accuracy: 0.8468\n",
            "Epoch 23/40\n",
            "30/30 [==============================] - 5s 168ms/step - loss: 2.1680e-04 - accuracy: 1.0000 - val_loss: 2.3232 - val_accuracy: 0.8484\n",
            "Epoch 24/40\n",
            "30/30 [==============================] - 5s 167ms/step - loss: 8.2464e-05 - accuracy: 1.0000 - val_loss: 2.3471 - val_accuracy: 0.8471\n",
            "Epoch 25/40\n",
            "30/30 [==============================] - 5s 167ms/step - loss: 2.1403e-04 - accuracy: 1.0000 - val_loss: 2.3849 - val_accuracy: 0.8481\n",
            "Epoch 26/40\n",
            "30/30 [==============================] - 5s 168ms/step - loss: 5.8552e-05 - accuracy: 1.0000 - val_loss: 2.4096 - val_accuracy: 0.8477\n",
            "Epoch 27/40\n",
            "30/30 [==============================] - 5s 167ms/step - loss: 4.5653e-05 - accuracy: 1.0000 - val_loss: 2.4258 - val_accuracy: 0.8480\n",
            "Epoch 28/40\n",
            "30/30 [==============================] - 5s 167ms/step - loss: 4.8287e-05 - accuracy: 1.0000 - val_loss: 2.4417 - val_accuracy: 0.8473\n",
            "Epoch 29/40\n",
            "30/30 [==============================] - 5s 167ms/step - loss: 5.1484e-05 - accuracy: 1.0000 - val_loss: 2.4602 - val_accuracy: 0.8482\n",
            "Epoch 30/40\n",
            "30/30 [==============================] - 5s 167ms/step - loss: 3.4321e-05 - accuracy: 1.0000 - val_loss: 2.4759 - val_accuracy: 0.8474\n",
            "Epoch 31/40\n",
            "30/30 [==============================] - 5s 167ms/step - loss: 3.1594e-05 - accuracy: 1.0000 - val_loss: 2.4905 - val_accuracy: 0.8478\n",
            "Epoch 32/40\n",
            "30/30 [==============================] - 5s 166ms/step - loss: 2.9328e-05 - accuracy: 1.0000 - val_loss: 2.5051 - val_accuracy: 0.8478\n",
            "Epoch 33/40\n",
            "30/30 [==============================] - 5s 167ms/step - loss: 2.7786e-05 - accuracy: 1.0000 - val_loss: 2.5195 - val_accuracy: 0.8475\n",
            "Epoch 34/40\n",
            "30/30 [==============================] - 5s 167ms/step - loss: 2.5142e-05 - accuracy: 1.0000 - val_loss: 2.5308 - val_accuracy: 0.8475\n",
            "Epoch 35/40\n",
            "30/30 [==============================] - 5s 167ms/step - loss: 2.3401e-05 - accuracy: 1.0000 - val_loss: 2.5446 - val_accuracy: 0.8478\n",
            "Epoch 36/40\n",
            "30/30 [==============================] - 5s 167ms/step - loss: 2.1712e-05 - accuracy: 1.0000 - val_loss: 2.5545 - val_accuracy: 0.8473\n",
            "Epoch 37/40\n",
            "30/30 [==============================] - 6s 188ms/step - loss: 2.1128e-05 - accuracy: 1.0000 - val_loss: 2.5670 - val_accuracy: 0.8478\n",
            "Epoch 38/40\n",
            "30/30 [==============================] - 5s 167ms/step - loss: 2.0845e-05 - accuracy: 1.0000 - val_loss: 2.5785 - val_accuracy: 0.8476\n",
            "Epoch 39/40\n",
            "30/30 [==============================] - 5s 167ms/step - loss: 1.9097e-05 - accuracy: 1.0000 - val_loss: 2.5904 - val_accuracy: 0.8478\n",
            "Epoch 40/40\n",
            "30/30 [==============================] - 5s 166ms/step - loss: 1.7474e-05 - accuracy: 1.0000 - val_loss: 2.6008 - val_accuracy: 0.8475\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 2.8493 - accuracy: 0.8292\n",
            "[2.849324941635132, 0.829200029373169]\n"
          ]
        }
      ],
      "source": [
        "# Train and test the model\n",
        "# your code goes here\n",
        "model6.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model6.fit(partial_X_train, partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)\n",
        "\n",
        "results = model6.evaluate(X_test_enc, y_test)\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "8QsvE6z5vakL",
        "outputId": "08e02921-425d-4a07-d933-36263321edb0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# your code goes here\n",
        "plt.plot(history.history['loss'], linestyle=\"\",marker=\"o\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx--Ytk3ZbLo"
      },
      "source": [
        "Although adding layer here reduces the training loss, the evaluation accuracy is worse than the model without the extra convolutional layer.\n",
        "\n",
        "Adding more layers can help you to extract more features. But we can do that up to a certain extent. After some point, instead of extracting features, we tend to overfit the data. Overfitting can lead to errors in on form or another, such as false positives. It is not easy to choose the number of units in a hidden layer or the number of hidden layers in a neural network. For many applications, one hidden layer is enough. As a general rule, the number of units in that hidden layer is between the number of inputs and the number of outputs.\n",
        " The best way to decide on the number of units and hidden layers is to try various parameters. Train several neural networks with different numbers of hidden layers and neurons, and monitor the performance of them. You will have to experiment using a series of different architectures. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn2GSV4ioyO2"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "c8gIzXncfaJK",
        "i_9a_rybhG5J",
        "7GdY2-64YG1B"
      ],
      "machine_shape": "hm",
      "name": "Lab3_mySolution.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}